"""
é”™è¯¯åˆ†ææ¨¡å—

è´Ÿè´£äººï¼šæˆå‘˜Eï¼ˆæ•°æ®å·¥ç¨‹ï¼‰

åŠŸèƒ½ï¼š
1. åˆ†æé”™è¯¯æ¡ˆä¾‹çš„æ¨¡å¼
2. è¯†åˆ«å“ªä¸€æ­¥æ¨ç†å‡ºé”™
3. åŒºåˆ†æ•°æ®é—®é¢˜å’Œæ¨ç†é—®é¢˜
4. ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š
"""
import json
from typing import List, Dict, Any, Optional
from collections import Counter, defaultdict
from pathlib import Path
from dataclasses import dataclass, field
from datetime import datetime


@dataclass
class ErrorCase:
    """é”™è¯¯æ¡ˆä¾‹"""
    id: int
    xungu_sentence: str
    beishi_char: str
    shi_char: str
    context: Optional[str]
    source: str
    predicted: str
    expected: str
    reasoning: str
    step_results: Dict[str, Any] = field(default_factory=dict)


@dataclass 
class ErrorPattern:
    """é”™è¯¯æ¨¡å¼"""
    pattern_type: str
    description: str
    cases: List[ErrorCase]
    frequency: int
    suggestions: List[str]


class ErrorAnalyzer:
    """
    é”™è¯¯åˆ†æå™¨
    
    åˆ†æAgentæ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œæ‰¾å‡ºé”™è¯¯æ¨¡å¼å’Œæ”¹è¿›å»ºè®®
    """
    
    def __init__(self):
        self.errors: List[ErrorCase] = []
        self.patterns: List[ErrorPattern] = []
    
    def add_errors(self, errors: List[Dict[str, Any]]) -> None:
        """æ·»åŠ é”™è¯¯æ¡ˆä¾‹"""
        for err in errors:
            case = ErrorCase(
                id=err.get("id", 0),
                xungu_sentence=err.get("è®­è¯‚å¥", ""),
                beishi_char=err.get("è¢«é‡Šå­—", ""),
                shi_char=err.get("é‡Šå­—", ""),
                context=err.get("ä¸Šä¸‹æ–‡"),
                source=err.get("å‡ºå¤„", ""),
                predicted=err.get("é¢„æµ‹", ""),
                expected=err.get("æ­£ç¡®", ""),
                reasoning=err.get("æ¨ç†", ""),
                step_results=err.get("äº”æ­¥åˆ†æ", {})
            )
            self.errors.append(case)
    
    def analyze_all(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢é”™è¯¯åˆ†æ"""
        if not self.errors:
            return {"message": "æ²¡æœ‰é”™è¯¯æ¡ˆä¾‹éœ€è¦åˆ†æ"}
        
        analysis = {
            "total_errors": len(self.errors),
            "error_type_distribution": self._analyze_error_types(),
            "step_error_analysis": self._analyze_step_errors(),
            "pattern_analysis": self._identify_patterns(),
            "source_analysis": self._analyze_by_source(),
            "context_analysis": self._analyze_context_impact(),
            "suggestions": self._generate_suggestions()
        }
        
        return analysis
    
    def _analyze_error_types(self) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯ç±»å‹åˆ†å¸ƒ"""
        # å‡å€Ÿè¯¯åˆ¤ä¸ºè¯­ä¹‰ vs è¯­ä¹‰è¯¯åˆ¤ä¸ºå‡å€Ÿ
        jiajie_to_yuyi = sum(1 for e in self.errors 
                            if e.expected == "å‡å€Ÿè¯´æ˜" and e.predicted == "è¯­ä¹‰è§£é‡Š")
        yuyi_to_jiajie = sum(1 for e in self.errors
                            if e.expected == "è¯­ä¹‰è§£é‡Š" and e.predicted == "å‡å€Ÿè¯´æ˜")
        
        return {
            "å‡å€Ÿè¯¯åˆ¤ä¸ºè¯­ä¹‰": jiajie_to_yuyi,
            "è¯­ä¹‰è¯¯åˆ¤ä¸ºå‡å€Ÿ": yuyi_to_jiajie,
            "æ¯”ä¾‹": {
                "å‡å€Ÿè¯¯åˆ¤ç‡": jiajie_to_yuyi / len(self.errors) if self.errors else 0,
                "è¯­ä¹‰è¯¯åˆ¤ç‡": yuyi_to_jiajie / len(self.errors) if self.errors else 0
            }
        }
    
    def _analyze_step_errors(self) -> Dict[str, Any]:
        """åˆ†æå„æ­¥éª¤çš„é”™è¯¯è´¡çŒ®"""
        step_issues = defaultdict(int)
        step_contributions = defaultdict(list)
        
        for err in self.errors:
            steps = err.step_results
            if not steps:
                continue
            
            # åˆ†ææ¯ä¸€æ­¥çš„é—®é¢˜
            # Step 1: è¯­ä¹‰åˆ†æ
            if "è¯­ä¹‰" in steps:
                semantic = steps["è¯­ä¹‰"]
                if err.expected == "å‡å€Ÿè¯´æ˜":
                    # å‡å€Ÿæ¡ˆä¾‹ï¼Œè¯­ä¹‰åº”è¯¥æ˜¯"ä¹‰è¿œ"
                    if semantic.get("relation") == "ä¹‰è¿‘":
                        step_issues["step1_è¯­ä¹‰è¯¯åˆ¤"] += 1
                        step_contributions["step1"].append(err.id)
            
            # Step 2: éŸ³éŸµåˆ†æ
            if "éŸ³éŸµ" in steps:
                phonetic = steps["éŸ³éŸµ"]
                # æ— è®ºå‡å€Ÿè¿˜æ˜¯è¯­ä¹‰ï¼Œå¾ˆå¤šæƒ…å†µä¸‹åº”è¯¥æ˜¯éŸ³è¿‘
                if phonetic.get("relation") == "éŸ³è¿œ":
                    step_issues["step2_éŸ³éŸµæ•°æ®ç¼ºå¤±æˆ–è¯¯åˆ¤"] += 1
                    step_contributions["step2"].append(err.id)
            
            # Step 4: æœ¯è¯­åˆ†æ
            if "æœ¯è¯­" in steps:
                pattern = steps["æœ¯è¯­"]
                if pattern.get("direct_judge"):
                    # å¦‚æœæœ¯è¯­ç›´æ¥åˆ¤æ–­ä½†ç»“æœé”™è¯¯
                    step_issues["step4_æœ¯è¯­åˆ¤æ–­å¤±è¯¯"] += 1
                    step_contributions["step4"].append(err.id)
            
            # Step 5: è¯­å¢ƒåˆ†æ
            if "è¯­å¢ƒ" in steps:
                context = steps["è¯­å¢ƒ"]
                if err.context and context.get("conclusion") != "ä¸ç¡®å®š":
                    # æœ‰è¯­å¢ƒä½†ç»“è®ºä¸é¢„æœŸä¸ç¬¦
                    if err.expected == "å‡å€Ÿè¯´æ˜" and context.get("conclusion") != "æ”¯æŒå‡å€Ÿ":
                        step_issues["step5_è¯­å¢ƒåˆ†æå¤±è¯¯"] += 1
                        step_contributions["step5"].append(err.id)
                    elif err.expected == "è¯­ä¹‰è§£é‡Š" and context.get("conclusion") != "æ”¯æŒè¯­ä¹‰":
                        step_issues["step5_è¯­å¢ƒåˆ†æå¤±è¯¯"] += 1
                        step_contributions["step5"].append(err.id)
        
        return {
            "step_issues": dict(step_issues),
            "step_contributions": {k: len(v) for k, v in step_contributions.items()},
            "most_problematic_step": max(step_contributions.keys(), 
                                         key=lambda k: len(step_contributions[k])) if step_contributions else None
        }
    
    def _identify_patterns(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«å¸¸è§é”™è¯¯æ¨¡å¼"""
        patterns = []
        
        # æ¨¡å¼1: æœ¯è¯­å¯¼å‘é”™è¯¯ - "è¯»ä¸º/è¯»æ›°"ç­‰æœ¯è¯­æ²¡æœ‰è¢«æ­£ç¡®è¯†åˆ«
        pattern1_cases = []
        for err in self.errors:
            if err.expected == "å‡å€Ÿè¯´æ˜":
                keywords = ["è¯»ä¸º", "è¯»æ›°", "è¯»å¦‚", "é€š"]
                if any(kw in err.xungu_sentence for kw in keywords):
                    pattern1_cases.append(err)
        
        if pattern1_cases:
            patterns.append({
                "pattern_type": "å‡å€Ÿæœ¯è¯­æœªè¯†åˆ«",
                "description": "è®­è¯‚å¥ä¸­åŒ…å«'è¯»ä¸º/è¯»æ›°/è¯»å¦‚/é€š'ç­‰å‡å€Ÿä¸“ç”¨æœ¯è¯­ï¼Œä½†æœªè¢«æ­£ç¡®åˆ¤æ–­ä¸ºå‡å€Ÿ",
                "frequency": len(pattern1_cases),
                "case_ids": [e.id for e in pattern1_cases],
                "suggestions": [
                    "åŠ å¼ºPattern Toolå¯¹å‡å€Ÿæœ¯è¯­çš„è¯†åˆ«è§„åˆ™",
                    "æé«˜å‡å€Ÿæœ¯è¯­çš„åˆ¤æ–­æƒé‡"
                ]
            })
        
        # æ¨¡å¼2: è¯­ä¹‰ç›¸è¿‘è¯¯åˆ¤ - è¢«é‡Šå­—å’Œé‡Šå­—è¯­ä¹‰æ¥è¿‘ä½†å®é™…æ˜¯å‡å€Ÿ
        pattern2_cases = []
        for err in self.errors:
            if err.expected == "å‡å€Ÿè¯´æ˜" and err.predicted == "è¯­ä¹‰è§£é‡Š":
                # è¿™äº›æ¡ˆä¾‹å¯èƒ½æ˜¯è¯­ä¹‰çœ‹èµ·æ¥ç›¸è¿‘ä½†å®é™…æ˜¯å‡å€Ÿ
                pattern2_cases.append(err)
        
        if pattern2_cases:
            patterns.append({
                "pattern_type": "è¯­ä¹‰ç›¸è¿‘å‡å€Ÿè¯¯åˆ¤",
                "description": "è¢«é‡Šå­—ä¸é‡Šå­—è¡¨é¢è¯­ä¹‰ç›¸è¿‘ï¼Œä½†å®é™…æ˜¯å‡å€Ÿå…³ç³»ï¼Œè¢«è¯¯åˆ¤ä¸ºè¯­ä¹‰è§£é‡Š",
                "frequency": len(pattern2_cases),
                "case_ids": [e.id for e in pattern2_cases],
                "suggestions": [
                    "éœ€è¦æ›´æ·±å…¥çš„è¯­ä¹‰åˆ†æï¼ŒåŒºåˆ†'è¡¨é¢è¯­ä¹‰'å’Œ'æœ¬ä¹‰'",
                    "å¢åŠ å¼‚æ–‡ä½è¯çš„æƒé‡"
                ]
            })
        
        # æ¨¡å¼3: éŸ³éŸµæ•°æ®ç¼ºå¤±
        pattern3_cases = []
        for err in self.errors:
            steps = err.step_results
            if "éŸ³éŸµ" in steps:
                phonetic = steps["éŸ³éŸµ"]
                if not phonetic.get("found_a") or not phonetic.get("found_b"):
                    pattern3_cases.append(err)
        
        if pattern3_cases:
            patterns.append({
                "pattern_type": "éŸ³éŸµæ•°æ®ç¼ºå¤±",
                "description": "è¢«é‡Šå­—æˆ–é‡Šå­—çš„éŸ³éŸµæ•°æ®ç¼ºå¤±ï¼Œå¯¼è‡´éŸ³éŸµåˆ¤æ–­ä¸å‡†ç¡®",
                "frequency": len(pattern3_cases),
                "case_ids": [e.id for e in pattern3_cases],
                "suggestions": [
                    "æ‰©å……éŸ³éŸµæ•°æ®åº“",
                    "å¯¹äºç¼ºå¤±æ•°æ®çš„å­—ï¼Œä½¿ç”¨ç›¸è¿‘å­—çš„éŸ³éŸµç‰¹å¾"
                ]
            })
        
        # æ¨¡å¼4: æ— è¯­å¢ƒä¾èµ–
        pattern4_cases = []
        for err in self.errors:
            if not err.context:
                pattern4_cases.append(err)
        
        if pattern4_cases:
            patterns.append({
                "pattern_type": "ç¼ºå°‘è¯­å¢ƒä¿¡æ¯",
                "description": "æµ‹è¯•æ¡ˆä¾‹ç¼ºå°‘ä¸Šä¸‹æ–‡è¯­å¢ƒï¼Œå½±å“åˆ¤æ–­å‡†ç¡®æ€§",
                "frequency": len(pattern4_cases),
                "case_ids": [e.id for e in pattern4_cases],
                "suggestions": [
                    "è¡¥å……æµ‹è¯•æ¡ˆä¾‹çš„è¯­å¢ƒä¿¡æ¯",
                    "å¯¹äºæ— è¯­å¢ƒæ¡ˆä¾‹ï¼Œå¢åŠ å…¶ä»–æ­¥éª¤çš„åˆ¤æ–­æƒé‡"
                ]
            })
        
        self.patterns = patterns
        return patterns
    
    def _analyze_by_source(self) -> Dict[str, Any]:
        """æŒ‰æ¥æºåˆ†æé”™è¯¯"""
        source_errors = defaultdict(list)
        for err in self.errors:
            # æå–ä¸»è¦æ¥æº
            source = err.source
            if 'ã€‹' in source:
                source = source.split('ã€‹')[0] + 'ã€‹'
            source_errors[source].append(err.id)
        
        return {
            "source_distribution": {k: len(v) for k, v in source_errors.items()},
            "most_error_source": max(source_errors.keys(), 
                                     key=lambda k: len(source_errors[k])) if source_errors else None
        }
    
    def _analyze_context_impact(self) -> Dict[str, Any]:
        """åˆ†æè¯­å¢ƒå¯¹å‡†ç¡®ç‡çš„å½±å“"""
        with_context = [e for e in self.errors if e.context]
        without_context = [e for e in self.errors if not e.context]
        
        return {
            "æœ‰è¯­å¢ƒé”™è¯¯æ•°": len(with_context),
            "æ— è¯­å¢ƒé”™è¯¯æ•°": len(without_context),
            "æœ‰è¯­å¢ƒé”™è¯¯ID": [e.id for e in with_context],
            "æ— è¯­å¢ƒé”™è¯¯ID": [e.id for e in without_context]
        }
    
    def _generate_suggestions(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        error_types = self._analyze_error_types()
        
        if error_types["å‡å€Ÿè¯¯åˆ¤ä¸ºè¯­ä¹‰"] > error_types["è¯­ä¹‰è¯¯åˆ¤ä¸ºå‡å€Ÿ"]:
            suggestions.append("å»ºè®®1: åŠ å¼ºå‡å€Ÿç±»å‹çš„è¯†åˆ«èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨ä»¥ä¸‹ç‰¹å¾ï¼š")
            suggestions.append("  - 'è¯»ä¸º/è¯»æ›°/è¯»å¦‚'ç­‰ä¸“ç”¨æœ¯è¯­")
            suggestions.append("  - ä¹‰è¿œä½†éŸ³è¿‘çš„å­—å¯¹")
            suggestions.append("  - æœ‰å¼‚æ–‡ä½è¯çš„æ¡ˆä¾‹")
        else:
            suggestions.append("å»ºè®®1: åŠ å¼ºè¯­ä¹‰è§£é‡Šç±»å‹çš„è¯†åˆ«èƒ½åŠ›ï¼Œç‰¹åˆ«å…³æ³¨ï¼š")
            suggestions.append("  - åŒå£°å éŸµä¸”ä¹‰è¿‘çš„å­—å¯¹")
            suggestions.append("  - 'ä¹‹ä¸ºè¨€'ç­‰è¯­æºè®­é‡Šæœ¯è¯­")
        
        # æ ¹æ®æ­¥éª¤åˆ†æ
        step_analysis = self._analyze_step_errors()
        problematic_step = step_analysis.get("most_problematic_step")
        if problematic_step:
            step_names = {
                "step1": "è¯­ä¹‰åˆ†æ",
                "step2": "éŸ³éŸµåˆ†æ", 
                "step3": "å¼‚æ–‡ä½è¯",
                "step4": "æœ¯è¯­è¯†åˆ«",
                "step5": "è¯­å¢ƒåˆ†æ"
            }
            suggestions.append(f"å»ºè®®2: é‡ç‚¹ä¼˜åŒ–{step_names.get(problematic_step, problematic_step)}æ¨¡å—")
        
        # æ•°æ®å»ºè®®
        suggestions.append("å»ºè®®3: æ•°æ®å±‚é¢æ”¹è¿›ï¼š")
        suggestions.append("  - æ‰©å……éŸ³éŸµæ•°æ®åº“çš„è¦†ç›–èŒƒå›´")
        suggestions.append("  - å¢åŠ è¯å…¸ä¸­å‡å€Ÿæ ‡æ³¨çš„æå–")
        suggestions.append("  - è¡¥å……æµ‹è¯•æ¡ˆä¾‹çš„è¯­å¢ƒä¿¡æ¯")
        
        return suggestions
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„é”™è¯¯åˆ†ææŠ¥å‘Š"""
        analysis = self.analyze_all()
        
        report = {
            "generated_at": datetime.now().isoformat(),
            "summary": {
                "total_errors": len(self.errors),
                "error_types": analysis["error_type_distribution"]
            },
            "detailed_analysis": {
                "step_errors": analysis["step_error_analysis"],
                "patterns": analysis["pattern_analysis"],
                "source_analysis": analysis["source_analysis"],
                "context_impact": analysis["context_analysis"]
            },
            "suggestions": analysis["suggestions"],
            "error_cases": [
                {
                    "id": e.id,
                    "è®­è¯‚å¥": e.xungu_sentence,
                    "è¢«é‡Šå­—": e.beishi_char,
                    "é‡Šå­—": e.shi_char,
                    "é¢„æµ‹": e.predicted,
                    "æ­£ç¡®": e.expected,
                    "æ¨ç†": e.reasoning
                }
                for e in self.errors
            ]
        }
        
        return report
    
    def print_report(self) -> None:
        """æ‰“å°é”™è¯¯åˆ†ææŠ¥å‘Š"""
        analysis = self.analyze_all()
        
        print("\n" + "=" * 70)
        print("é”™è¯¯åˆ†ææŠ¥å‘Š")
        print("=" * 70)
        
        print(f"\nğŸ“Š é”™è¯¯æ¦‚è§ˆ")
        print(f"  æ€»é”™è¯¯æ•°: {len(self.errors)}")
        
        error_types = analysis["error_type_distribution"]
        print(f"\nğŸ“‹ é”™è¯¯ç±»å‹åˆ†å¸ƒ")
        print(f"  å‡å€Ÿè¯¯åˆ¤ä¸ºè¯­ä¹‰: {error_types['å‡å€Ÿè¯¯åˆ¤ä¸ºè¯­ä¹‰']}")
        print(f"  è¯­ä¹‰è¯¯åˆ¤ä¸ºå‡å€Ÿ: {error_types['è¯­ä¹‰è¯¯åˆ¤ä¸ºå‡å€Ÿ']}")
        
        step_analysis = analysis["step_error_analysis"]
        print(f"\nğŸ” æ­¥éª¤é”™è¯¯åˆ†æ")
        for step, count in step_analysis.get("step_issues", {}).items():
            print(f"  {step}: {count}")
        if step_analysis.get("most_problematic_step"):
            print(f"  æœ€éœ€æ”¹è¿›çš„æ­¥éª¤: {step_analysis['most_problematic_step']}")
        
        print(f"\nğŸ“ é”™è¯¯æ¨¡å¼")
        for pattern in analysis.get("pattern_analysis", []):
            print(f"\n  [{pattern['pattern_type']}] (å‡ºç°{pattern['frequency']}æ¬¡)")
            print(f"    æè¿°: {pattern['description']}")
            print(f"    å»ºè®®: {', '.join(pattern['suggestions'][:2])}")
        
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®")
        for suggestion in analysis.get("suggestions", []):
            print(f"  {suggestion}")
        
        print("\n" + "=" * 70)


def save_error_report(report: Dict[str, Any], filepath: str) -> None:
    """ä¿å­˜é”™è¯¯åˆ†ææŠ¥å‘Š"""
    path = Path(filepath)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"é”™è¯¯åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")


# ===== æµ‹è¯•ä»£ç  =====

if __name__ == "__main__":
    # æ¨¡æ‹Ÿä¸€äº›é”™è¯¯æ¡ˆä¾‹
    mock_errors = [
        {
            "id": 1,
            "è®­è¯‚å¥": "å´‡ï¼Œç»ˆä¹Ÿ",
            "è¢«é‡Šå­—": "å´‡",
            "é‡Šå­—": "ç»ˆ",
            "ä¸Šä¸‹æ–‡": "å´‡æœå…¶é›¨",
            "å‡ºå¤„": "ã€Šè¯—Â·é‚¶é£Â·ç®€å…®ã€‹ã€Šæ¯›ä¼ ã€‹",
            "é¢„æµ‹": "è¯­ä¹‰è§£é‡Š",
            "æ­£ç¡®": "å‡å€Ÿè¯´æ˜",
            "æ¨ç†": "å´‡ä¸ç»ˆè¯­ä¹‰ç›¸å…³ï¼Œæ•…åˆ¤æ–­ä¸ºè¯­ä¹‰è§£é‡Š",
            "äº”æ­¥åˆ†æ": {
                "è¯­ä¹‰": {"relation": "ä¹‰è¿‘"},
                "éŸ³éŸµ": {"relation": "éŸ³è¿‘", "found_a": True, "found_b": True},
                "æœ¯è¯­": {"direct_judge": False},
                "è¯­å¢ƒ": {"conclusion": "ä¸ç¡®å®š"}
            }
        },
        {
            "id": 2,
            "è®­è¯‚å¥": "æ­£ï¼Œè¯»ä¸ºå¾",
            "è¢«é‡Šå­—": "æ­£",
            "é‡Šå­—": "å¾",
            "ä¸Šä¸‹æ–‡": "æ­£å…¶è´§è´¿",
            "å‡ºå¤„": "ã€Šå‘¨ç¤¼Â·åœ°å®˜Â·å¸é—¨ã€‹éƒ‘ç„æ³¨",
            "é¢„æµ‹": "è¯­ä¹‰è§£é‡Š",
            "æ­£ç¡®": "å‡å€Ÿè¯´æ˜",
            "æ¨ç†": "æ­£ä¸å¾å­˜åœ¨è¯­ä¹‰å…³è”",
            "äº”æ­¥åˆ†æ": {
                "è¯­ä¹‰": {"relation": "ä¹‰è¿‘"},
                "éŸ³éŸµ": {"relation": "éŸ³è¿‘", "found_a": True, "found_b": True},
                "æœ¯è¯­": {"direct_judge": True, "pattern": "è¯»ä¸º"},
                "è¯­å¢ƒ": {"conclusion": "æ”¯æŒå‡å€Ÿ"}
            }
        },
        {
            "id": 3,
            "è®­è¯‚å¥": "æ”¿ï¼Œæ­£ä¹Ÿ",
            "è¢«é‡Šå­—": "æ”¿",
            "é‡Šå­—": "æ­£",
            "ä¸Šä¸‹æ–‡": None,
            "å‡ºå¤„": "ã€Šå¹¿é›…Â·é‡Šè¯‚ã€‹",
            "é¢„æµ‹": "å‡å€Ÿè¯´æ˜",
            "æ­£ç¡®": "è¯­ä¹‰è§£é‡Š",
            "æ¨ç†": "æ”¿ä¸æ­£åŒéŸ³ï¼Œå¯èƒ½æ˜¯å‡å€Ÿ",
            "äº”æ­¥åˆ†æ": {
                "è¯­ä¹‰": {"relation": "ä¹‰è¿‘"},
                "éŸ³éŸµ": {"relation": "éŸ³è¿‘", "found_a": True, "found_b": True},
                "æœ¯è¯­": {"direct_judge": False},
                "è¯­å¢ƒ": {"conclusion": "ä¸ç¡®å®š"}
            }
        }
    ]
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ErrorAnalyzer()
    analyzer.add_errors(mock_errors)
    
    # æ‰“å°æŠ¥å‘Š
    analyzer.print_report()
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    report = analyzer.generate_report()
    
    # ä¿å­˜ç¤ºä¾‹
    project_root = Path(__file__).parent.parent.parent
    output_path = project_root / "data/processed/error_analysis_sample.json"
    save_error_report(report, str(output_path))
