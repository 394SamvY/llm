"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—

è´Ÿè´£äººï¼šæˆå‘˜Eï¼ˆæ•°æ®å·¥ç¨‹ï¼‰

åŠŸèƒ½ï¼š
1. è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼
2. ç”Ÿæˆæ··æ·†çŸ©é˜µ
3. é”™è¯¯åˆ†ææŠ¥å‘Š
4. æ”¯æŒä»JSONæ–‡ä»¶åŠ è½½æµ‹è¯•é›†
"""
import json
from typing import List, Dict, Any, Tuple, Optional
from collections import Counter
from pathlib import Path
from dataclasses import dataclass


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹"""
    id: int
    xungu_sentence: str
    beishi_char: str
    shi_char: str
    context: Optional[str]
    source: str
    expected_label: str
    notes: str = ""


def load_test_dataset(filepath: str) -> List[TestCase]:
    """
    ä»JSONæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®é›†
    
    Args:
        filepath: JSONæ–‡ä»¶è·¯å¾„
    
    Returns:
        TestCaseåˆ—è¡¨
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    cases = []
    for item in data:
        case = TestCase(
            id=item.get("id", 0),
            xungu_sentence=item.get("è®­è¯‚å¥", ""),
            beishi_char=item.get("è¢«é‡Šå­—", ""),
            shi_char=item.get("é‡Šå­—", ""),
            context=item.get("ä¸Šä¸‹æ–‡"),
            source=item.get("å‡ºå¤„", ""),
            expected_label=item.get("æ­£ç¡®ç­”æ¡ˆ", ""),
            notes=item.get("å¤‡æ³¨", "")
        )
        cases.append(case)
    
    return cases


def calculate_metrics(
    predictions: List[str],
    labels: List[str]
) -> Dict[str, float]:
    """
    è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    
    Args:
        predictions: é¢„æµ‹æ ‡ç­¾åˆ—è¡¨
        labels: çœŸå®æ ‡ç­¾åˆ—è¡¨
        
    Returns:
        dict: {
            "accuracy": å‡†ç¡®ç‡,
            "precision_å‡å€Ÿ": å‡å€Ÿçš„ç²¾ç¡®ç‡,
            "recall_å‡å€Ÿ": å‡å€Ÿçš„å¬å›ç‡,
            "f1_å‡å€Ÿ": å‡å€Ÿçš„F1,
            "precision_è¯­ä¹‰": è¯­ä¹‰çš„ç²¾ç¡®ç‡,
            "recall_è¯­ä¹‰": è¯­ä¹‰çš„å¬å›ç‡,
            "f1_è¯­ä¹‰": è¯­ä¹‰çš„F1,
            "macro_f1": å®å¹³å‡F1
        }
    """
    assert len(predictions) == len(labels), "é¢„æµ‹å’Œæ ‡ç­¾æ•°é‡ä¸ä¸€è‡´"
    
    n = len(predictions)
    if n == 0:
        return {"accuracy": 0.0}
    
    # å‡†ç¡®ç‡
    correct = sum(p == l for p, l in zip(predictions, labels))
    accuracy = correct / n
    
    # åˆ†ç±»åˆ«ç»Ÿè®¡
    results = {}
    for label_type in ["å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š"]:
        tp = sum(1 for p, l in zip(predictions, labels) if p == label_type and l == label_type)
        fp = sum(1 for p, l in zip(predictions, labels) if p == label_type and l != label_type)
        fn = sum(1 for p, l in zip(predictions, labels) if p != label_type and l == label_type)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        short_name = "å‡å€Ÿ" if "å‡å€Ÿ" in label_type else "è¯­ä¹‰"
        results[f"precision_{short_name}"] = precision
        results[f"recall_{short_name}"] = recall
        results[f"f1_{short_name}"] = f1
        results[f"tp_{short_name}"] = tp
        results[f"fp_{short_name}"] = fp
        results[f"fn_{short_name}"] = fn
    
    # å®å¹³å‡F1
    macro_f1 = (results.get("f1_å‡å€Ÿ", 0) + results.get("f1_è¯­ä¹‰", 0)) / 2
    
    return {
        "accuracy": accuracy,
        "correct": correct,
        "total": n,
        **results,
        "macro_f1": macro_f1
    }


def build_confusion_matrix(
    predictions: List[str],
    labels: List[str],
    label_names: List[str] = None
) -> Dict[str, Any]:
    """
    æ„å»ºæ··æ·†çŸ©é˜µ
    
    Args:
        predictions: é¢„æµ‹æ ‡ç­¾åˆ—è¡¨
        labels: çœŸå®æ ‡ç­¾åˆ—è¡¨
        label_names: æ ‡ç­¾åç§°åˆ—è¡¨
    
    Returns:
        {
            "matrix": [[TP, FN], [FP, TN]],
            "labels": ["å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š"],
            "normalized": [[...], [...]]  # å½’ä¸€åŒ–åçš„çŸ©é˜µ
        }
    """
    if label_names is None:
        label_names = ["å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š"]
    
    n_labels = len(label_names)
    matrix = [[0] * n_labels for _ in range(n_labels)]
    
    label_to_idx = {name: i for i, name in enumerate(label_names)}
    
    for pred, label in zip(predictions, labels):
        if pred in label_to_idx and label in label_to_idx:
            i = label_to_idx[label]  # çœŸå®æ ‡ç­¾
            j = label_to_idx[pred]    # é¢„æµ‹æ ‡ç­¾
            matrix[i][j] += 1
    
    # å½’ä¸€åŒ–
    normalized = []
    for row in matrix:
        row_sum = sum(row)
        if row_sum > 0:
            normalized.append([x / row_sum for x in row])
        else:
            normalized.append([0.0] * n_labels)
    
    return {
        "matrix": matrix,
        "labels": label_names,
        "normalized": normalized
    }


def print_confusion_matrix(cm: Dict[str, Any]) -> None:
    """æ‰“å°æ··æ·†çŸ©é˜µ"""
    labels = cm["labels"]
    matrix = cm["matrix"]
    
    # è®¡ç®—åˆ—å®½
    max_label_len = max(len(l) for l in labels)
    col_width = max(max_label_len + 2, 8)
    
    print("\næ··æ·†çŸ©é˜µ:")
    print("-" * (col_width * (len(labels) + 1) + 10))
    
    # æ ‡é¢˜è¡Œ
    header = " " * (max_label_len + 2) + "â”‚"
    for label in labels:
        header += f" {label:^{col_width}} â”‚"
    print(header)
    print("-" * (col_width * (len(labels) + 1) + 10))
    
    # æ•°æ®è¡Œ
    for i, label in enumerate(labels):
        row = f" {label:<{max_label_len}} â”‚"
        for j in range(len(labels)):
            row += f" {matrix[i][j]:^{col_width}} â”‚"
        print(row)
    
    print("-" * (col_width * (len(labels) + 1) + 10))
    
    # æ‰“å°å½’ä¸€åŒ–çŸ©é˜µ
    print("\nå½’ä¸€åŒ–æ··æ·†çŸ©é˜µ (æŒ‰è¡Œ):")
    normalized = cm["normalized"]
    for i, label in enumerate(labels):
        row = f" {label:<{max_label_len}} â”‚"
        for j in range(len(labels)):
            row += f" {normalized[i][j]:^{col_width}.2%} â”‚"
        print(row)


def evaluate_results(
    results: List[Dict[str, Any]],
    dataset: List[TestCase]
) -> Dict[str, Any]:
    """
    è¯„ä¼°Agentçš„åˆ†æç»“æœ
    
    Args:
        results: Agentè¿”å›çš„ç»“æœåˆ—è¡¨
        dataset: æµ‹è¯•æ•°æ®é›†
        
    Returns:
        dict: è¯„ä¼°æŠ¥å‘Š
    """
    predictions = []
    labels = []
    errors = []
    
    for result, case in zip(results, dataset):
        pred = result.get("classification", "")
        label = case.expected_label
        
        predictions.append(pred)
        labels.append(label)
        
        if pred != label:
            errors.append({
                "id": case.id,
                "è®­è¯‚å¥": case.xungu_sentence,
                "è¢«é‡Šå­—": case.beishi_char,
                "é‡Šå­—": case.shi_char,
                "ä¸Šä¸‹æ–‡": case.context,
                "å‡ºå¤„": case.source,
                "é¢„æµ‹": pred,
                "æ­£ç¡®": label,
                "æ¨ç†": result.get("final_reasoning", ""),
                "äº”æ­¥åˆ†æ": {
                    "è¯­ä¹‰": result.get("step1", {}),
                    "éŸ³éŸµ": result.get("step2", {}),
                    "å¼‚æ–‡": result.get("step3", {}),
                    "æœ¯è¯­": result.get("step4", {}),
                    "è¯­å¢ƒ": result.get("step5", {})
                }
            })
    
    metrics = calculate_metrics(predictions, labels)
    confusion = build_confusion_matrix(predictions, labels)
    
    return {
        "metrics": metrics,
        "confusion_matrix": confusion,
        "total": len(predictions),
        "correct": sum(p == l for p, l in zip(predictions, labels)),
        "predictions": predictions,
        "labels": labels,
        "errors": errors
    }


def print_evaluation_report(report: Dict[str, Any]) -> None:
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("è®­è¯‚åˆ†ç±»è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    
    metrics = report["metrics"]
    print(f"\nğŸ“Š æ€»ä½“æŒ‡æ ‡")
    print(f"  æ€»æ ·æœ¬æ•°: {report['total']}")
    print(f"  æ­£ç¡®æ•°: {report['correct']}")
    print(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
    print(f"  å®å¹³å‡F1: {metrics.get('macro_f1', 0):.2%}")
    
    print(f"\nğŸ“— å‡å€Ÿè¯´æ˜")
    print(f"  ç²¾ç¡®ç‡: {metrics.get('precision_å‡å€Ÿ', 0):.2%}")
    print(f"  å¬å›ç‡: {metrics.get('recall_å‡å€Ÿ', 0):.2%}")
    print(f"  F1: {metrics.get('f1_å‡å€Ÿ', 0):.2%}")
    print(f"  TP/FP/FN: {metrics.get('tp_å‡å€Ÿ', 0)}/{metrics.get('fp_å‡å€Ÿ', 0)}/{metrics.get('fn_å‡å€Ÿ', 0)}")
    
    print(f"\nğŸ“˜ è¯­ä¹‰è§£é‡Š")
    print(f"  ç²¾ç¡®ç‡: {metrics.get('precision_è¯­ä¹‰', 0):.2%}")
    print(f"  å¬å›ç‡: {metrics.get('recall_è¯­ä¹‰', 0):.2%}")
    print(f"  F1: {metrics.get('f1_è¯­ä¹‰', 0):.2%}")
    print(f"  TP/FP/FN: {metrics.get('tp_è¯­ä¹‰', 0)}/{metrics.get('fp_è¯­ä¹‰', 0)}/{metrics.get('fn_è¯­ä¹‰', 0)}")
    
    # æ··æ·†çŸ©é˜µ
    if "confusion_matrix" in report:
        print_confusion_matrix(report["confusion_matrix"])
    
    # é”™è¯¯æ¡ˆä¾‹
    if report["errors"]:
        print(f"\nâŒ é”™è¯¯æ¡ˆä¾‹ ({len(report['errors'])} ä¸ª)")
        print("-" * 60)
        for err in report["errors"][:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"\n[æ¡ˆä¾‹ {err['id']}] {err['è®­è¯‚å¥']}")
            print(f"  è¢«é‡Šå­—: {err['è¢«é‡Šå­—']}, é‡Šå­—: {err['é‡Šå­—']}")
            if err.get('ä¸Šä¸‹æ–‡'):
                print(f"  ä¸Šä¸‹æ–‡: {err['ä¸Šä¸‹æ–‡']}")
            print(f"  å‡ºå¤„: {err['å‡ºå¤„']}")
            print(f"  é¢„æµ‹: {err['é¢„æµ‹']} âœ— â†’ æ­£ç¡®: {err['æ­£ç¡®']} âœ“")
            if err.get('æ¨ç†'):
                print(f"  æ¨ç†: {err['æ¨ç†'][:100]}...")


def save_evaluation_report(report: Dict[str, Any], filepath: str) -> None:
    """ä¿å­˜è¯„ä¼°æŠ¥å‘Šåˆ°JSONæ–‡ä»¶"""
    path = Path(filepath)
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„éƒ¨åˆ†
    output = {
        "metrics": report["metrics"],
        "confusion_matrix": report["confusion_matrix"],
        "total": report["total"],
        "correct": report["correct"],
        "error_count": len(report.get("errors", [])),
        "errors": report.get("errors", [])[:20]  # åªä¿å­˜å‰20ä¸ªé”™è¯¯
    }
    
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")


def get_dataset_statistics(dataset: List[TestCase]) -> Dict[str, Any]:
    """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    label_counts = Counter(case.expected_label for case in dataset)
    source_counts = Counter(case.source.split('ã€‹')[0] + 'ã€‹' if 'ã€‹' in case.source else case.source for case in dataset)
    
    return {
        "total": len(dataset),
        "label_distribution": dict(label_counts),
        "source_distribution": dict(source_counts.most_common(10)),
        "with_context": sum(1 for case in dataset if case.context),
        "without_context": sum(1 for case in dataset if not case.context)
    }


def print_dataset_statistics(stats: Dict[str, Any]) -> None:
    """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ•°æ®é›†ç»Ÿè®¡")
    print("=" * 60)
    
    print(f"\næ€»æ ·æœ¬æ•°: {stats['total']}")
    print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in stats['label_distribution'].items():
        pct = count / stats['total'] * 100
        print(f"  {label}: {count} ({pct:.1f}%)")
    
    print(f"\nä¸Šä¸‹æ–‡æƒ…å†µ:")
    print(f"  æœ‰ä¸Šä¸‹æ–‡: {stats['with_context']}")
    print(f"  æ— ä¸Šä¸‹æ–‡: {stats['without_context']}")
    
    print(f"\nä¸»è¦æ¥æº (å‰10):")
    for source, count in list(stats['source_distribution'].items())[:10]:
        print(f"  {source}: {count}")


# ===== ä¾¿æ·å‡½æ•° =====

def quick_evaluate(predictions: List[str], labels: List[str]) -> None:
    """å¿«é€Ÿè¯„ä¼°å¹¶æ‰“å°ç»“æœ"""
    metrics = calculate_metrics(predictions, labels)
    cm = build_confusion_matrix(predictions, labels)
    
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
    print(f"å®å¹³å‡F1: {metrics['macro_f1']:.2%}")
    print_confusion_matrix(cm)


# ===== æµ‹è¯•ä»£ç  =====

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†åŠ è½½
    project_root = Path(__file__).parent.parent.parent
    test_file = project_root / "data/test/test_dataset.json"
    
    if test_file.exists():
        print("åŠ è½½æµ‹è¯•æ•°æ®é›†...")
        dataset = load_test_dataset(str(test_file))
        stats = get_dataset_statistics(dataset)
        print_dataset_statistics(stats)
    
    # æµ‹è¯•æŒ‡æ ‡è®¡ç®—
    print("\n" + "=" * 60)
    print("æµ‹è¯•æŒ‡æ ‡è®¡ç®—")
    print("=" * 60)
    
    predictions = ["å‡å€Ÿè¯´æ˜", "å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š", "è¯­ä¹‰è§£é‡Š", "å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š"]
    labels = ["å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š", "è¯­ä¹‰è§£é‡Š", "è¯­ä¹‰è§£é‡Š", "å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š"]
    
    metrics = calculate_metrics(predictions, labels)
    print("\næŒ‡æ ‡ç»“æœ:")
    for k, v in metrics.items():
        if isinstance(v, float):
            print(f"  {k}: {v:.2%}")
        else:
            print(f"  {k}: {v}")
    
    # æµ‹è¯•æ··æ·†çŸ©é˜µ
    cm = build_confusion_matrix(predictions, labels)
    print_confusion_matrix(cm)
