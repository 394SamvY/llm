# æˆå‘˜E - æ•°æ®å·¥ç¨‹æ¥å£æ–‡æ¡£

> **è´Ÿè´£äºº**: æˆå‘˜E  
> **å®Œæˆæ—¥æœŸ**: 2026-01-17  
> **çŠ¶æ€**: âœ… å…¨éƒ¨å®Œæˆå¹¶æµ‹è¯•é€šè¿‡

---

## ğŸ“ æ–‡ä»¶ä½ç½®æ€»è§ˆ

```
llm25/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ phonology_unified.json    # éŸ³éŸµæ•°æ®ç´¢å¼• (13,666å­—)
â”‚   â”‚   â””â”€â”€ dyhdc_index.json          # è¯å…¸åç§»é‡ç´¢å¼• (24.5MB)
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ test_dataset.json         # æµ‹è¯•æ•°æ®é›† (60æ¡)
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ phonology_parser.py       # éŸ³éŸµæ•°æ®è§£æå™¨
    â”‚   â””â”€â”€ dyhdc_index_builder.py    # è¯å…¸ç´¢å¼•æ„å»ºå™¨
    â””â”€â”€ evaluation/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ metrics.py                # è¯„ä¼°æŒ‡æ ‡è®¡ç®—
        â”œâ”€â”€ test_dataset.py           # æµ‹è¯•æ•°æ®é›†ç±»
        â””â”€â”€ error_analysis.py         # é”™è¯¯åˆ†ææ¨¡å—
```

---

## ğŸ¯ æ¥å£1: éŸ³éŸµæ•°æ®æŸ¥è¯¢ (ç»™æˆå‘˜D - éŸ³éŸµå·¥å…·å¼€å‘)

### ä»»åŠ¡å†…å®¹
æä¾›ä¸Šå¤éŸ³éŸµæ•°æ®çš„æŸ¥è¯¢å’Œæ¯”è¾ƒåŠŸèƒ½ï¼Œæ”¯æŒåˆ¤æ–­ä¸¤å­—æ˜¯å¦"éŸ³è¿‘"ã€‚

### æ•°æ®æ¥æº
- æ½˜æ‚Ÿäº‘ã€Šæ±‰è¯­å¤éŸ³æ‰‹å†Œã€‹: 13,445 å­—
- ç™½ä¸€å¹³-æ²™åŠ å°”ä¸Šå¤éŸ³: 4,056 å­—
- æ•´åˆåå…±: **13,666** ä¸ªå­—

### ä½¿ç”¨æ–¹å¼

```python
from src.data import load_phonology_data, compare_phonology

# 1. åŠ è½½éŸ³éŸµæ•°æ®
phonology_data = load_phonology_data("data/processed/phonology_unified.json")

# 2. æŸ¥è¯¢å•ä¸ªå­—çš„éŸ³éŸµä¿¡æ¯
char_info = phonology_data.get("å´‡")
# è¿”å›:
# {
#     "å­—": "å´‡",
#     "æ½˜æ‚Ÿäº‘": {
#         "éŸµéƒ¨": "çµ‚",
#         "ä¸Šå¤éŸ³": "*dzruÅ‹",
#         "å£°æ¯": "å´‡",
#         "éŸµ": "é¾",
#         "å£°è°ƒ": "å¹³"
#     },
#     "ç™½ä¸€å¹³æ²™åŠ å°”": {
#         "ä¸Šå¤éŸ³": "*[dz]<r>uÅ‹",
#         "ä¸­å¤éŸ³": "dzywng",
#         "é‡Šä¹‰": "high; lofty"
#     }
# }

# 3. æ¯”è¾ƒä¸¤å­—çš„éŸ³éŸµå…³ç³»
result = compare_phonology("æµ·", "æ™¦", phonology_data)
# è¿”å›:
# {
#     "char_a": "æµ·",
#     "char_b": "æ™¦",
#     "found_a": True,
#     "found_b": True,
#     "éŸ³è¿‘": True,           # ç»¼åˆåˆ¤æ–­ç»“æœ
#     "éŸµéƒ¨ç›¸åŒ": True,       # éŸµéƒ¨æ˜¯å¦ç›¸åŒ
#     "å£°æ¯ç›¸è¿‘": True,       # å£°æ¯æ˜¯å¦ç›¸è¿‘
#     "è¯¦æƒ…": {
#         "æ½˜æ‚Ÿäº‘": {
#             "éŸµéƒ¨_A": "ä¹‹", "éŸµéƒ¨_B": "ä¹‹",
#             "å£°æ¯_A": "æ›‰", "å£°æ¯_B": "æ›‰",
#             "ä¸Šå¤éŸ³_A": "mÌ¥Ê°É¯Ì Ê”", "ä¸Šå¤éŸ³_B": "mÌ¥Ê°É¯Ì s"
#         }
#     }
# }
```

### âš ï¸ æ³¨æ„äº‹é¡¹
1. **ç¹ç®€ä½“é—®é¢˜**: éŸ³éŸµæ•°æ®ä½¿ç”¨ç¹ä½“å­—ç´¢å¼•ï¼Œå¦‚æŸ¥"ç»ˆ"éœ€ç”¨"çµ‚"
2. **ç¼ºå¤±æ•°æ®**: è‹¥å­—ä¸åœ¨æ•°æ®åº“ä¸­ï¼Œ`found_a/found_b`ä¼šè¿”å›`False`
3. **éŸ³è¿‘åˆ¤æ–­**: éŸµéƒ¨ç›¸åŒæˆ–å£°æ¯ç›¸è¿‘ï¼Œåˆ™ç»¼åˆåˆ¤æ–­ä¸º"éŸ³è¿‘"

---

## ğŸ¯ æ¥å£2: è¯å…¸è¯­ä¹‰æŸ¥è¯¢ (ç»™æˆå‘˜C - è¯­ä¹‰å·¥å…·å¼€å‘)

### ä»»åŠ¡å†…å®¹
æä¾›ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹çš„å¿«é€ŸæŸ¥è¯¢åŠŸèƒ½ï¼Œæ”¯æŒæŸ¥è¯¢å­—çš„æœ¬ä¹‰ã€ä¹‰é¡¹ã€å‡å€Ÿæ ‡æ³¨ç­‰ã€‚

### æ•°æ®è§„æ¨¡
- è¯æ¡æ€»æ•°: 408,931 æ¡
- é¦–å­—æ•°: 27,678 ä¸ª
- ç´¢å¼•å¤§å°: 24.5 MB

### ä½¿ç”¨æ–¹å¼

```python
from src.data import DYHDCIndexLoader

# 1. åˆå§‹åŒ–åŠ è½½å™¨
loader = DYHDCIndexLoader(
    jsonl_path="ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹ç»“æ„åŒ–/dyhdc.parsed.fixed.v2.jsonl",
    index_path="data/processed/dyhdc_index.json"
)

# 2. æŸ¥è¯¢å•ä¸ªå­—çš„è¯­ä¹‰ä¿¡æ¯
result = loader.query_single_char("å´‡")
# è¿”å›:
# {
#     "å­—": "å´‡",
#     "ç®€ä½“": "",
#     "è¯»éŸ³": "chÃ³ng",
#     "æœ¬ä¹‰": "1é«˜ï¼›é«˜å¤§ã€‚",
#     "ä¹‰é¡¹": [
#         "é«˜ï¼›é«˜å¤§",
#         "å°Šå´‡",
#         "å……å®",
#         ...
#     ],  # æœ€å¤š10ä¸ªä¹‰é¡¹
#     "ä¾‹å¥": [
#         "å´‡å±±å³»å²­",
#         ...
#     ],  # æœ€å¤š5ä¸ªä¾‹å¥
#     "å‡å€Ÿæ ‡æ³¨": [
#         "12é€š"çµ‚"ã€‚ç»ˆå°½ã€‚å‚è§"å´‡æœ"ã€‚"
#     ]  # è¯å…¸ä¸­æ ‡æ³¨çš„å‡å€Ÿ/é€šå‡ä¿¡æ¯
# }
```

### âš ï¸ æ³¨æ„äº‹é¡¹
1. **ç¹ä½“å­—**: è¯å…¸ä½¿ç”¨ç¹ä½“å­—ï¼ŒæŸ¥"ç»ˆ"éœ€ç”¨"çµ‚"
2. **é¦–æ¬¡åŠ è½½**: é¦–æ¬¡æŸ¥è¯¢ä¼šåŠ è½½ç´¢å¼•ï¼ˆçº¦1ç§’ï¼‰
3. **å‡å€Ÿæ ‡æ³¨**: `å‡å€Ÿæ ‡æ³¨`å­—æ®µåŒ…å«è¯å…¸ä¸­"è¯»ä¸º"ã€"é€š"ç­‰å‡å€Ÿè¯´æ˜

---

## ğŸ¯ æ¥å£3: æµ‹è¯•æ•°æ®é›† (ç»™æˆå‘˜B - Agentå¼€å‘)

### ä»»åŠ¡å†…å®¹
æä¾›æ ‡æ³¨å¥½çš„è®­è¯‚å¥æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°Agentçš„åˆ†ç±»å‡†ç¡®ç‡ã€‚

### æ•°æ®ç»Ÿè®¡
- æ€»æ ·æœ¬: 60 æ¡
- å‡å€Ÿè¯´æ˜: 17 æ¡ (28.3%)
- è¯­ä¹‰è§£é‡Š: 43 æ¡ (71.7%)
- æœ‰ä¸Šä¸‹æ–‡: 30 æ¡
- æ— ä¸Šä¸‹æ–‡: 30 æ¡

### ä½¿ç”¨æ–¹å¼

```python
from src.evaluation import load_test_dataset, get_dataset_statistics

# 1. åŠ è½½æµ‹è¯•æ•°æ®é›†
dataset = load_test_dataset("data/test/test_dataset.json")

# 2. æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡
stats = get_dataset_statistics(dataset)
# è¿”å›:
# {
#     "total": 60,
#     "label_distribution": {"å‡å€Ÿè¯´æ˜": 17, "è¯­ä¹‰è§£é‡Š": 43},
#     "with_context": 30,
#     "without_context": 30,
#     "source_distribution": {...}
# }

# 3. éå†æµ‹è¯•ç”¨ä¾‹
for case in dataset:
    print(f"ID: {case.id}")
    print(f"è®­è¯‚å¥: {case.xungu_sentence}")
    print(f"è¢«é‡Šå­—: {case.beishi_char}")
    print(f"é‡Šå­—: {case.shi_char}")
    print(f"ä¸Šä¸‹æ–‡: {case.context}")
    print(f"å‡ºå¤„: {case.source}")
    print(f"æ­£ç¡®ç­”æ¡ˆ: {case.expected_label}")
    print(f"å¤‡æ³¨: {case.notes}")
```

### æµ‹è¯•ç”¨ä¾‹æ•°æ®ç»“æ„ (JSON)

```json
{
    "id": 1,
    "è®­è¯‚å¥": "å´‡ï¼Œç»ˆä¹Ÿ",
    "è¢«é‡Šå­—": "å´‡",
    "é‡Šå­—": "ç»ˆ",
    "ä¸Šä¸‹æ–‡": "å´‡æœå…¶é›¨",
    "å‡ºå¤„": "ã€Šè¯—Â·é‚¶é£Â·ç®€å…®ã€‹ã€Šæ¯›ä¼ ã€‹",
    "æ­£ç¡®ç­”æ¡ˆ": "å‡å€Ÿè¯´æ˜",
    "å¤‡æ³¨": "æœ‰å¼‚æ–‡ã€Šå°é›…Â·é‡‡ç»¿ã€‹ä½œ'ç»ˆæœ'..."
}
```

### æ ‡ç­¾è¯´æ˜
| æ ‡ç­¾ | å«ä¹‰ | ç‰¹å¾ |
|------|------|------|
| `å‡å€Ÿè¯´æ˜` | å€Ÿå­—ä¸æ­£å­—çš„å…³ç³» | ä¹‰è¿œéŸ³è¿‘ï¼Œæœ‰"è¯»ä¸º/è¯»æ›°"ç­‰æœ¯è¯­ |
| `è¯­ä¹‰è§£é‡Š` | é€šè¿‡å£°éŸ³è§£é‡Šè¯­ä¹‰/è¯­æº | ä¹‰è¿‘éŸ³è¿‘ï¼Œæœ‰"ä¹‹ä¸ºè¨€"ç­‰æœ¯è¯­ |

---

## ğŸ¯ æ¥å£4: è¯„ä¼°æŒ‡æ ‡è®¡ç®— (ç»™æˆå‘˜B - Agentå¼€å‘)

### ä»»åŠ¡å†…å®¹
è®¡ç®—åˆ†ç±»ä»»åŠ¡çš„å„é¡¹è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ã€æ··æ·†çŸ©é˜µã€‚

### ä½¿ç”¨æ–¹å¼

```python
from src.evaluation import (
    calculate_metrics,
    build_confusion_matrix,
    print_confusion_matrix,
    evaluate_results,
    print_evaluation_report
)

# æ–¹å¼1: ç›´æ¥è®¡ç®—æŒ‡æ ‡
predictions = ["å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š", "è¯­ä¹‰è§£é‡Š", ...]
labels = ["å‡å€Ÿè¯´æ˜", "å‡å€Ÿè¯´æ˜", "è¯­ä¹‰è§£é‡Š", ...]

metrics = calculate_metrics(predictions, labels)
# è¿”å›:
# {
#     "accuracy": 0.833,           # å‡†ç¡®ç‡
#     "correct": 5,                # æ­£ç¡®æ•°
#     "total": 6,                  # æ€»æ•°
#     "precision_å‡å€Ÿ": 0.667,     # å‡å€Ÿç²¾ç¡®ç‡
#     "recall_å‡å€Ÿ": 1.0,          # å‡å€Ÿå¬å›ç‡
#     "f1_å‡å€Ÿ": 0.8,              # å‡å€ŸF1
#     "precision_è¯­ä¹‰": 1.0,       # è¯­ä¹‰ç²¾ç¡®ç‡
#     "recall_è¯­ä¹‰": 0.75,         # è¯­ä¹‰å¬å›ç‡
#     "f1_è¯­ä¹‰": 0.857,            # è¯­ä¹‰F1
#     "macro_f1": 0.829            # å®å¹³å‡F1
# }

# æ–¹å¼2: ç”Ÿæˆæ··æ·†çŸ©é˜µ
cm = build_confusion_matrix(predictions, labels)
print_confusion_matrix(cm)
# è¾“å‡º:
# æ··æ·†çŸ©é˜µ:
#              â”‚   å‡å€Ÿè¯´æ˜   â”‚   è¯­ä¹‰è§£é‡Š   â”‚
# ------------------------------------------------
#  å‡å€Ÿè¯´æ˜    â”‚      2      â”‚      0      â”‚
#  è¯­ä¹‰è§£é‡Š    â”‚      1      â”‚      3      â”‚

# æ–¹å¼3: å®Œæ•´è¯„ä¼°Agentç»“æœ
agent_results = [
    {
        "classification": "å‡å€Ÿè¯´æ˜",
        "final_reasoning": "...",
        "step1": {...},  # äº”æ­¥åˆ†æç»“æœ
        ...
    },
    ...
]

report = evaluate_results(agent_results, dataset)
print_evaluation_report(report)
```

---

## ğŸ¯ æ¥å£5: é”™è¯¯åˆ†æ (ç»™æˆå‘˜B - Agentå¼€å‘)

### ä»»åŠ¡å†…å®¹
åˆ†æAgentçš„é”™è¯¯æ¡ˆä¾‹ï¼Œæ‰¾å‡ºé”™è¯¯æ¨¡å¼å’Œæ”¹è¿›å»ºè®®ã€‚

### ä½¿ç”¨æ–¹å¼

```python
from src.evaluation import ErrorAnalyzer, save_error_report

# 1. åˆ›å»ºåˆ†æå™¨
analyzer = ErrorAnalyzer()

# 2. æ·»åŠ é”™è¯¯æ¡ˆä¾‹ (ä»evaluate_resultsçš„report["errors"]è·å–)
analyzer.add_errors(report["errors"])

# 3. æ‰§è¡Œåˆ†æ
analysis = analyzer.analyze_all()
# è¿”å›:
# {
#     "total_errors": 3,
#     "error_type_distribution": {
#         "å‡å€Ÿè¯¯åˆ¤ä¸ºè¯­ä¹‰": 2,
#         "è¯­ä¹‰è¯¯åˆ¤ä¸ºå‡å€Ÿ": 1
#     },
#     "step_error_analysis": {
#         "step_issues": {"step1_è¯­ä¹‰è¯¯åˆ¤": 2, ...},
#         "most_problematic_step": "step1"
#     },
#     "pattern_analysis": [...],
#     "suggestions": [...]
# }

# 4. æ‰“å°æŠ¥å‘Š
analyzer.print_report()

# 5. ä¿å­˜æŠ¥å‘Š
report = analyzer.generate_report()
save_error_report(report, "data/processed/error_analysis.json")
```

### é”™è¯¯æ¨¡å¼ç±»å‹
| æ¨¡å¼ | æè¿° |
|------|------|
| `å‡å€Ÿæœ¯è¯­æœªè¯†åˆ«` | å«"è¯»ä¸º/è¯»æ›°"ç­‰æœ¯è¯­ä½†æœªåˆ¤æ–­ä¸ºå‡å€Ÿ |
| `è¯­ä¹‰ç›¸è¿‘å‡å€Ÿè¯¯åˆ¤` | è¡¨é¢è¯­ä¹‰ç›¸è¿‘çš„å‡å€Ÿè¢«è¯¯åˆ¤ä¸ºè¯­ä¹‰è§£é‡Š |
| `éŸ³éŸµæ•°æ®ç¼ºå¤±` | å­—çš„éŸ³éŸµæ•°æ®ç¼ºå¤±å¯¼è‡´åˆ¤æ–­ä¸å‡† |
| `ç¼ºå°‘è¯­å¢ƒä¿¡æ¯` | æ— ä¸Šä¸‹æ–‡å½±å“åˆ¤æ–­å‡†ç¡®æ€§ |

---

## ğŸ“‹ Agentè¾“å‡ºæ ¼å¼å»ºè®® (ç»™æˆå‘˜A - æ¶æ„è®¾è®¡)

å»ºè®®Agentçš„åˆ†æç»“æœé‡‡ç”¨ä»¥ä¸‹ç»Ÿä¸€æ ¼å¼ï¼Œä¾¿äºè¯„ä¼°å’Œé”™è¯¯åˆ†æï¼š

```python
{
    "classification": "å‡å€Ÿè¯´æ˜" | "è¯­ä¹‰è§£é‡Š",
    "confidence": 0.0 - 1.0,
    "final_reasoning": "ç»¼åˆåˆ¤æ–­ç†ç”±...",
    
    # äº”æ­¥åˆ†æç»“æœ
    "step1": {  # è¯­ä¹‰åˆ†æ
        "relation": "ä¹‰è¿‘" | "ä¹‰è¿œ",
        "confidence": 0.8,
        "meaning_a": "é«˜å¤§",
        "meaning_b": "ç»ˆç»“",
        "reasoning": "..."
    },
    "step2": {  # éŸ³éŸµåˆ†æ
        "relation": "éŸ³è¿‘" | "éŸ³è¿œ",
        "éŸµéƒ¨ç›¸åŒ": True,
        "å£°æ¯ç›¸è¿‘": False,
        "confidence": 0.9
    },
    "step3": {  # å¼‚æ–‡/æ–‡ä¾‹ä½è¯
        "has_evidence": True,
        "evidence": "ã€Šå°é›…Â·é‡‡ç»¿ã€‹ä½œ'ç»ˆæœ'"
    },
    "step4": {  # æœ¯è¯­/è®­å¼åˆ†æ
        "matched_pattern": "Aä¹Ÿ" | "è¯»ä¸º" | "ä¹‹ä¸ºè¨€" | ...,
        "direct_judge": True | False,
        "implied_type": "å‡å€Ÿè¯´æ˜" | "è¯­ä¹‰è§£é‡Š" | "ä¸ç¡®å®š"
    },
    "step5": {  # è¯­å¢ƒåˆ†æ
        "conclusion": "æ”¯æŒå‡å€Ÿ" | "æ”¯æŒè¯­ä¹‰" | "ä¸ç¡®å®š",
        "beishi_fit": False,
        "shi_fit": True,
        "reasoning": "..."
    }
}
```

---

## ğŸ”§ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

```python
"""å®Œæ•´ä½¿ç”¨ç¤ºä¾‹"""
from src.data import load_phonology_data, compare_phonology, DYHDCIndexLoader
from src.evaluation import load_test_dataset, evaluate_results, print_evaluation_report

# 1. åŠ è½½æ‰€æœ‰æ•°æ®
phonology = load_phonology_data("data/processed/phonology_unified.json")
dict_loader = DYHDCIndexLoader(
    "ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹ç»“æ„åŒ–/dyhdc.parsed.fixed.v2.jsonl",
    "data/processed/dyhdc_index.json"
)
test_dataset = load_test_dataset("data/test/test_dataset.json")

# 2. ä½¿ç”¨æ•°æ® (ç¤ºä¾‹: åˆ†æ"å´‡ï¼Œç»ˆä¹Ÿ")
beishi, shi = "å´‡", "çµ‚"

# æŸ¥éŸ³éŸµ
phonetic_result = compare_phonology(beishi, shi, phonology)
print(f"éŸ³è¿‘: {phonetic_result['éŸ³è¿‘']}")

# æŸ¥è¯­ä¹‰
semantic_a = dict_loader.query_single_char(beishi)
semantic_b = dict_loader.query_single_char(shi)
print(f"å´‡çš„æœ¬ä¹‰: {semantic_a['æœ¬ä¹‰']}")
print(f"çµ‚çš„æœ¬ä¹‰: {semantic_b['æœ¬ä¹‰']}")
print(f"å‡å€Ÿæ ‡æ³¨: {semantic_a.get('å‡å€Ÿæ ‡æ³¨', [])}")

# 3. è¯„ä¼°Agent (å‡è®¾æœ‰agent_results)
# report = evaluate_results(agent_results, test_dataset)
# print_evaluation_report(report)
```

---

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰æ¥å£ä½¿ç”¨é—®é¢˜ï¼Œè¯·è”ç³»æˆå‘˜Eã€‚
