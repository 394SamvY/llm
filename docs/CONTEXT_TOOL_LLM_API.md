# Context Tool LLM API æ¥å…¥è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

`context_tool.py` å·²ç»**å®Œå…¨æ¥å…¥ LLM API**ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨ï¼š

## ğŸ”§ æ¥å…¥æ–¹å¼

### æ–¹å¼1ï¼šæ‰‹åŠ¨åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆæ¨èï¼‰

```python
from openai import OpenAI
from src.tools.context_tool import ContextTool

# åˆ›å»ºLLMå®¢æˆ·ç«¯
llm_client = OpenAI(
    base_url="https://api.tokenpony.cn/v1",
    api_key="sk-0d73949767524be2989b35415d2ccbe0"
)
llm_client._model = "qwen3-coder-480b"

# åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆä¼ å…¥llm_clientï¼‰
tool = ContextTool(llm_client=llm_client)

# ä½¿ç”¨å·¥å…·
result = tool.analyze(
    original_sentence="å´‡æœå…¶é›¨",
    char_a="å´‡",
    char_b="çµ‚",
    meaning_a="é«˜ï¼›é«˜å¤§",
    meaning_b="ç»ˆç»“ã€æ•´ä¸ª"
)

print(result.conclusion)  # "æ”¯æŒå‡å€Ÿ"
```

### æ–¹å¼2ï¼šè‡ªåŠ¨åˆå§‹åŒ–ï¼ˆä½¿ç”¨å†…ç½®é…ç½®ï¼‰

```python
from src.tools.context_tool import ContextTool

# ä½¿ç”¨ auto_init=Trueï¼Œä¼šè‡ªåŠ¨ä»é…ç½®åˆ›å»ºå®¢æˆ·ç«¯
tool = ContextTool(auto_init=True)

# ä½¿ç”¨å·¥å…·
result = tool.analyze(
    original_sentence="å´‡æœå…¶é›¨",
    char_a="å´‡",
    char_b="çµ‚",
    meaning_a="é«˜ï¼›é«˜å¤§",
    meaning_b="ç»ˆç»“ã€æ•´ä¸ª"
)
```

### æ–¹å¼3ï¼šå‡½æ•°å¼æ¥å£

```python
from openai import OpenAI
from src.tools.context_tool import analyze_context

# åˆ›å»ºå®¢æˆ·ç«¯
llm_client = OpenAI(
    base_url="https://api.tokenpony.cn/v1",
    api_key="sk-0d73949767524be2989b35415d2ccbe0"
)
llm_client._model = "qwen3-coder-480b"

# ç›´æ¥è°ƒç”¨å‡½æ•°
result = analyze_context(
    original_sentence="å´‡æœå…¶é›¨",
    char_a="å´‡",
    char_b="çµ‚",
    meaning_a="é«˜ï¼›é«˜å¤§",
    meaning_b="ç»ˆç»“ã€æ•´ä¸ª",
    llm_client=llm_client
)

print(result["ç»“è®º"])  # "æ”¯æŒå‡å€Ÿ"
```

## ğŸ” ä»£ç å®ç°è¯´æ˜

### 1. LLM API è°ƒç”¨å®ç°

åœ¨ `_analyze_with_llm()` æ–¹æ³•ä¸­ï¼ˆç¬¬107-140è¡Œï¼‰ï¼š

```python
def _analyze_with_llm(self, ...) -> ContextAnalysis:
    """ä½¿ç”¨LLMè¿›è¡Œåˆ†æ"""
    prompt = self._build_prompt(...)
    
    # è°ƒç”¨LLM API
    response = self.llm_client.chat.completions.create(
        model=getattr(self.llm_client, '_model', 'qwen3-coder-480b'),
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        max_tokens=500
    )
    
    # è§£æå“åº”
    content = response.choices[0].message.content
    return self._parse_response(content, ...)
```

### 2. è‡ªåŠ¨å®¢æˆ·ç«¯åˆ›å»º

åœ¨ `_create_client_from_config()` æ–¹æ³•ä¸­ï¼ˆç¬¬53-75è¡Œï¼‰ï¼š

```python
def _create_client_from_config(self):
    """ä»é…ç½®åˆ›å»ºLLMå®¢æˆ·ç«¯"""
    from openai import OpenAI
    
    # ä½¿ç”¨ç¡¬ç¼–ç çš„APIé…ç½®ï¼ˆå·²åŒ…å«æ‚¨çš„API keyï¼‰
    api_key = "sk-0d73949767524be2989b35415d2ccbe0"
    base_url = "https://api.tokenpony.cn/v1"
    model = "qwen3-coder-480b"
    
    client = OpenAI(base_url=base_url, api_key=api_key)
    client._model = model
    return client
```

### 3. æ™ºèƒ½å›é€€æœºåˆ¶

åœ¨ `analyze()` æ–¹æ³•ä¸­ï¼ˆç¬¬77-105è¡Œï¼‰ï¼š

```python
def analyze(self, ...) -> ContextAnalysis:
    """åˆ†æè¯­å¢ƒé€‚é…åº¦"""
    if self.llm_client is not None:
        # æœ‰LLMå®¢æˆ·ç«¯ â†’ ä½¿ç”¨LLM API
        return self._analyze_with_llm(...)
    else:
        # æ— LLMå®¢æˆ·ç«¯ â†’ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        return self._analyze_mock(...)
```

## ğŸ“Š å·¥ä½œæµç¨‹

```
ç”¨æˆ·è°ƒç”¨ analyze()
    â†“
æ£€æŸ¥æ˜¯å¦æœ‰ llm_client?
    â”œâ”€ æœ‰ â†’ _analyze_with_llm()
    â”‚         â†“
    â”‚     æ„å»ºæç¤ºè¯ (_build_prompt)
    â”‚         â†“
    â”‚     è°ƒç”¨LLM API (chat.completions.create)
    â”‚         â†“
    â”‚     è§£æå“åº” (_parse_response)
    â”‚         â†“
    â”‚     è¿”å› ContextAnalysis
    â”‚
    â””â”€ æ—  â†’ _analyze_mock()
              â†“
          è¿”å›é¢„è®¾çš„æ¨¡æ‹Ÿç»“æœ
```

## ğŸ§ª æµ‹è¯•éªŒè¯

è¿è¡Œæµ‹è¯•æ–‡ä»¶éªŒè¯LLM APIæ¥å…¥ï¼š

```bash
python tests/test_context_tool.py
```

æµ‹è¯•ä¼šï¼š
1. åˆ›å»ºLLMå®¢æˆ·ç«¯
2. è°ƒç”¨çœŸå®çš„LLM API
3. è§£æè¿”å›çš„JSONå“åº”
4. éªŒè¯åˆ†æç»“æœ

## âš™ï¸ é…ç½®è¯´æ˜

### APIé…ç½®ä½ç½®

ä»£ç ä¸­å·²ç¡¬ç¼–ç æ‚¨çš„APIé…ç½®ï¼ˆç¬¬62-64è¡Œï¼‰ï¼š

```python
api_key = settings.openai_api_key or "sk-0d73949767524be2989b35415d2ccbe0"
base_url = settings.openai_base_url or "https://api.tokenpony.cn/v1"
model = settings.llm_model or "qwen3-coder-480b"
```

### ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¯é€‰ï¼‰

ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼š

```bash
export OPENAI_API_KEY="sk-0d73949767524be2989b35415d2ccbe0"
export OPENAI_BASE_URL="https://api.tokenpony.cn/v1"
export LLM_MODEL="qwen3-coder-480b"
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºæœ¬ä½¿ç”¨

```python
from openai import OpenAI
from src.tools.context_tool import ContextTool

# åˆ›å»ºå®¢æˆ·ç«¯
client = OpenAI(
    base_url="https://api.tokenpony.cn/v1",
    api_key="sk-0d73949767524be2989b35415d2ccbe0"
)
client._model = "qwen3-coder-480b"

# åˆ›å»ºå·¥å…·
tool = ContextTool(llm_client=client)

# åˆ†æ
result = tool.analyze(
    original_sentence="å´‡æœå…¶é›¨",
    char_a="å´‡",
    char_b="çµ‚",
    meaning_a="é«˜ï¼›é«˜å¤§",
    meaning_b="ç»ˆç»“ã€æ•´ä¸ª"
)

print(f"ç»“è®º: {result.conclusion}")
print(f"ç†ç”±: {result.reasoning}")
```

### ç¤ºä¾‹2ï¼šåœ¨Agentä¸­ä½¿ç”¨

```python
from openai import OpenAI
from src.agent.xungu_agent import XunguAgent

# åˆ›å»ºLLMå®¢æˆ·ç«¯
llm_client = OpenAI(
    base_url="https://api.tokenpony.cn/v1",
    api_key="sk-0d73949767524be2989b35415d2ccbe0"
)
llm_client._model = "qwen3-coder-480b"

# åˆ›å»ºAgentï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨LLMè¿›è¡Œè¯­å¢ƒåˆ†æï¼‰
agent = XunguAgent(llm_client=llm_client)

# åˆ†æè®­è¯‚å¥
result = agent.analyze("å´‡ï¼Œçµ‚ä¹Ÿ", context="å´‡æœå…¶é›¨")
```

## âœ… æ¥å…¥çŠ¶æ€æ€»ç»“

| åŠŸèƒ½ | çŠ¶æ€ | è¯´æ˜ |
|------|:----:|------|
| LLM APIè°ƒç”¨ | âœ… å·²å®ç° | `_analyze_with_llm()` æ–¹æ³• |
| å“åº”è§£æ | âœ… å·²å®ç° | `_parse_response()` æ–¹æ³• |
| é”™è¯¯å¤„ç† | âœ… å·²å®ç° | å¤±è´¥æ—¶å›é€€åˆ°mock |
| è‡ªåŠ¨åˆå§‹åŒ– | âœ… å·²å®ç° | `auto_init=True` å‚æ•° |
| APIé…ç½® | âœ… å·²é…ç½® | ç¡¬ç¼–ç äº†æ‚¨çš„API keyå’Œbase_url |
| æµ‹è¯•æ–‡ä»¶ | âœ… å·²åˆ›å»º | `tests/test_context_tool.py` |

## ğŸ¯ å…³é”®ä»£ç ä½ç½®

- **LLMè°ƒç”¨**ï¼šç¬¬124-129è¡Œ
- **å“åº”è§£æ**ï¼šç¬¬142-200è¡Œ
- **å®¢æˆ·ç«¯åˆ›å»º**ï¼šç¬¬53-75è¡Œ
- **æç¤ºè¯æ„å»º**ï¼šç¬¬202-233è¡Œ

---

*æœ€åæ›´æ–°ï¼š2026-01-17*
