# ğŸ› ï¸ å®ç°æŒ‡å—

> æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å„æ¨¡å—çš„å…·ä½“å®ç°æ–¹æ³•å’Œä»£ç ç¤ºä¾‹

---

## ğŸ“‹ å®ç°ä¼˜å…ˆçº§

```
P0 - æ ¸å¿ƒå¿…é¡»ï¼ˆWeek 1-2ï¼‰
â”œâ”€â”€ âœ… é¡¹ç›®éª¨æ¶æ­å»º
â”œâ”€â”€ âœ… è®­å¼è¯†åˆ«å·¥å…·ï¼ˆStep 4ï¼‰
â”œâ”€â”€ â³ å­—ä¹‰æŸ¥è¯¢å·¥å…·ï¼ˆStep 1ï¼‰
â”œâ”€â”€ â³ éŸ³éŸµæŸ¥è¯¢å·¥å…·ï¼ˆStep 2ï¼‰
â””â”€â”€ â³ äº”æ­¥æ¨ç†æµç¨‹

P1 - é‡è¦åŠŸèƒ½ï¼ˆWeek 3ï¼‰
â”œâ”€â”€ â³ å¼‚æ–‡æ£€ç´¢å·¥å…·ï¼ˆStep 3ï¼‰
â”œâ”€â”€ â³ è¯­å¢ƒåˆ†æå·¥å…·ï¼ˆStep 5ï¼‰
â”œâ”€â”€ â³ æµ‹è¯•æ•°æ®é›†æ„å»º
â””â”€â”€ â³ è¯„ä¼°æŠ¥å‘Š

P2 - å¢å¼ºåŠŸèƒ½ï¼ˆWeek 4ï¼‰
â”œâ”€â”€ â³ æ¥å…¥ LLM API
â”œâ”€â”€ â³ ä¼˜åŒ–æç¤ºè¯
â”œâ”€â”€ â³ å‘é‡æ£€ç´¢
â””â”€â”€ â³ å¯è§†åŒ–ç•Œé¢
```

---

## ğŸ”§ Step 1: è¯­ä¹‰åˆ†æå·¥å…·

### ç›®æ ‡

æŸ¥è¯¢ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹è·å–å­—çš„æœ¬ä¹‰å’Œä¹‰é¡¹ï¼Œåˆ¤æ–­ä¸¤å­—æ˜¯å¦"ä¹‰è¿‘"æˆ–"ä¹‰è¿œ"ã€‚

### æ•°æ®æº

`ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹ç»“æ„åŒ–/dyhdc.parsed.fixed.v2.jsonl` (1.9GB)

### æ•°æ®ç»“æ„

```json
{
  "headword": "å´‡",
  "pronunciations": [
    {
      "pinyin": "chÃ³ng",
      "phonology": {"å£°æ¯": "èˆ¹", "éŸµæ¯": "ä¸œåˆä¸‰å¹³", "å£°è°ƒ": "å¹³"}
    }
  ],
  "senses": [
    {
      "sense_num": "1",
      "definition": "é«˜å¤§ã€‚",
      "examples": [
        {"source": "ã€Šè¯´æ–‡ã€‹", "quote": "å´‡ï¼Œåµ¬é«˜ä¹Ÿã€‚"}
      ]
    },
    {
      "sense_num": "2", 
      "definition": "å°Šå´‡ï¼Œå´‡æ•¬ã€‚",
      "examples": [...]
    }
  ]
}
```

### å®ç°ä»£ç 

```python
# src/knowledge/dictionary_loader.py

import json
from pathlib import Path
from typing import Dict, List, Optional


class DictionaryLoader:
    """ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, jsonl_path: str):
        self.jsonl_path = Path(jsonl_path)
        self.index: Dict[str, dict] = {}
        self._loaded = False
    
    def load(self, lazy: bool = True):
        """
        åŠ è½½æ•°æ®
        
        Args:
            lazy: True=æŒ‰éœ€åŠ è½½ï¼ŒFalse=å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
        """
        if lazy:
            # æ„å»ºè¡Œå·ç´¢å¼•ï¼ŒæŒ‰éœ€è¯»å–
            self._build_line_index()
        else:
            # å…¨éƒ¨åŠ è½½ï¼ˆéœ€è¦å¤§å†…å­˜ï¼‰
            self._load_all()
        self._loaded = True
    
    def _build_line_index(self):
        """æ„å»º headword -> è¡Œå· çš„ç´¢å¼•"""
        self._line_positions = {}
        with open(self.jsonl_path, 'r', encoding='utf-8') as f:
            position = 0
            for line in f:
                try:
                    data = json.loads(line.strip())
                    headword = data.get('headword', '')
                    if headword:
                        self._line_positions[headword] = position
                except:
                    pass
                position = f.tell()
    
    def query(self, char: str) -> Optional[dict]:
        """
        æŸ¥è¯¢å•å­—ä¿¡æ¯
        
        Args:
            char: è¦æŸ¥è¯¢çš„å­—
            
        Returns:
            å­—å…¸æ¡ç›®æˆ– None
        """
        if not self._loaded:
            self.load()
        
        # å…ˆæŸ¥å†…å­˜ç¼“å­˜
        if char in self.index:
            return self.index[char]
        
        # æŒ‰éœ€ä»æ–‡ä»¶è¯»å–
        if hasattr(self, '_line_positions') and char in self._line_positions:
            with open(self.jsonl_path, 'r', encoding='utf-8') as f:
                f.seek(self._line_positions[char])
                line = f.readline()
                data = json.loads(line.strip())
                self.index[char] = data  # ç¼“å­˜
                return data
        
        return None
    
    def get_primary_meaning(self, char: str) -> str:
        """è·å–å­—çš„æœ¬ä¹‰/é¦–è¦ä¹‰é¡¹"""
        data = self.query(char)
        if not data:
            return "æœªæ”¶å½•"
        
        senses = data.get('senses', [])
        if senses:
            # è¿”å›ç¬¬ä¸€ä¸ªä¹‰é¡¹ï¼ˆé€šå¸¸æ˜¯æœ¬ä¹‰ï¼‰
            return senses[0].get('definition', 'æœªçŸ¥')
        return "æœªçŸ¥"
```

```python
# src/tools/semantic_tool.py

from typing import Dict, Tuple
from ..knowledge.dictionary_loader import DictionaryLoader


class SemanticTool:
    """è¯­ä¹‰åˆ†æå·¥å…· - Step 1"""
    
    def __init__(self, dictionary_path: str = None):
        if dictionary_path:
            self.dictionary = DictionaryLoader(dictionary_path)
        else:
            self.dictionary = None
    
    def query_meaning(self, char: str) -> dict:
        """æŸ¥è¯¢å­—çš„è¯­ä¹‰ä¿¡æ¯"""
        if self.dictionary:
            data = self.dictionary.query(char)
            if data:
                return {
                    "char": char,
                    "found": True,
                    "meanings": data.get('senses', []),
                    "primary_meaning": self.dictionary.get_primary_meaning(char)
                }
        
        # è¿”å›æœªæ‰¾åˆ°
        return {
            "char": char,
            "found": False,
            "meanings": [],
            "primary_meaning": "æœªæ”¶å½•"
        }
    
    def compare_semantics(self, char_a: str, char_b: str) -> dict:
        """
        æ¯”è¾ƒä¸¤å­—è¯­ä¹‰å…³è”æ€§
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        - å¦‚æœä¸¤å­—æœ¬ä¹‰å±äºåŒä¸€è¯­ä¹‰åœº â†’ ä¹‰è¿‘
        - å¦‚æœä¸¤å­—æœ¬ä¹‰å®Œå…¨æ— å…³ â†’ ä¹‰è¿œ
        """
        meaning_a = self.query_meaning(char_a)
        meaning_b = self.query_meaning(char_b)
        
        # ç®€åŒ–åˆ¤æ–­é€»è¾‘ï¼ˆåç»­å¯æ¥å…¥LLMå¢å¼ºï¼‰
        # è¿™é‡Œä½¿ç”¨è§„åˆ™åˆ¤æ–­ï¼Œå®é™…åº”ç”¨ä¸­å¯ç”¨LLM
        
        primary_a = meaning_a.get("primary_meaning", "")
        primary_b = meaning_b.get("primary_meaning", "")
        
        # ç®€å•è§„åˆ™ï¼šå¦‚æœå®šä¹‰ä¸­åŒ…å«å¯¹æ–¹å­—ï¼Œå¯èƒ½ä¹‰è¿‘
        is_related = (
            char_b in primary_a or 
            char_a in primary_b or
            self._check_semantic_field(primary_a, primary_b)
        )
        
        return {
            "char_a": char_a,
            "char_b": char_b,
            "meaning_a": primary_a,
            "meaning_b": primary_b,
            "relation": "ä¹‰è¿‘" if is_related else "ä¹‰è¿œ",
            "confidence": 0.7 if is_related else 0.8,
            "reasoning": self._generate_reasoning(primary_a, primary_b, is_related)
        }
    
    def _check_semantic_field(self, meaning_a: str, meaning_b: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å±äºåŒä¸€è¯­ä¹‰åœºï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®šä¹‰ä¸€äº›è¯­ä¹‰åœºå…³é”®è¯
        semantic_fields = [
            ["é«˜", "å¤§", "å´‡", "å·¨", "ä¼Ÿ"],
            ["ç»ˆ", "æœ«", "å°½", "å®Œ", "æ¯•", "ç»“"],
            ["æš—", "é»‘", "æ™¦", "æ˜", "æš"],
            ["æ˜", "äº®", "å…‰", "è¾‰", "çš"],
            ["è¡Œ", "èµ°", "é“", "è·¯", "å¾„"],
            # ... å¯æ‰©å±•
        ]
        
        for field in semantic_fields:
            if any(w in meaning_a for w in field) and any(w in meaning_b for w in field):
                return True
        return False
    
    def _generate_reasoning(self, meaning_a: str, meaning_b: str, is_related: bool) -> str:
        if is_related:
            return f"'{meaning_a}'ä¸'{meaning_b}'å­˜åœ¨è¯­ä¹‰å…³è”"
        else:
            return f"'{meaning_a}'ä¸'{meaning_b}'è¯­ä¹‰æ— å…³è”"
```

---

## ğŸ”§ Step 2: è¯­éŸ³åˆ†æå·¥å…·

### ç›®æ ‡

æŸ¥è¯¢ä¸Šå¤éŸ³æ•°æ®ï¼Œåˆ¤æ–­ä¸¤å­—æ˜¯å¦"éŸ³è¿‘"ã€‚

### æ•°æ®æº

1. `éŸ³éŸµæ•°æ®/ä¸Šå¤éŸ³/æ½˜æ‚Ÿäº‘ã€Šæ±‰è¯­å¤éŸ³æ‰‹å†Œã€‹/æ±‰è¯­å¤éŸ³æ‰‹å†Œ.txt`
2. `éŸ³éŸµæ•°æ®/ä¸Šå¤éŸ³/æ–¯å¡”ç½—æ–¯é‡‘æ±‰è¯­æ‹ŸéŸ³/Chinese-characters.txt`
3. `éŸ³éŸµæ•°æ®/ä¸Šå¤éŸ³/ç™½ä¸€å¹³-æ²™åŠ å°”çš„æ±‰è¯­æ‹ŸéŸ³ä½“ç³»/BaxterSagartOC2015-10-13.xlsx`

### å®ç°ä»£ç 

```python
# src/knowledge/phonology_loader.py

import re
from pathlib import Path
from typing import Dict, Optional


class PhonologyLoader:
    """ä¸Šå¤éŸ³æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str = None):
        self.data: Dict[str, dict] = {}
        if data_dir:
            self._load_from_dir(Path(data_dir))
    
    def _load_from_dir(self, data_dir: Path):
        """ä»ç›®å½•åŠ è½½æ‰€æœ‰æ•°æ®æº"""
        # 1. åŠ è½½æ½˜æ‚Ÿäº‘æ•°æ®
        pan_path = data_dir / "æ½˜æ‚Ÿäº‘ã€Šæ±‰è¯­å¤éŸ³æ‰‹å†Œã€‹" / "æ±‰è¯­å¤éŸ³æ‰‹å†Œ.txt"
        if pan_path.exists():
            self._load_pan_wuyun(pan_path)
        
        # 2. åŠ è½½æ–¯å¡”ç½—æ–¯é‡‘æ•°æ®
        sta_path = data_dir / "æ–¯å¡”ç½—æ–¯é‡‘æ±‰è¯­æ‹ŸéŸ³" / "Chinese-characters.txt"
        if sta_path.exists():
            self._load_starostin(sta_path)
    
    def _load_pan_wuyun(self, path: Path):
        """åŠ è½½æ½˜æ‚Ÿäº‘ã€Šæ±‰è¯­å¤éŸ³æ‰‹å†Œã€‹"""
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # è§£ææ ¼å¼ï¼ˆéœ€æ ¹æ®å®é™…æ ¼å¼è°ƒæ•´ï¼‰
                parts = line.split('\t')
                if len(parts) >= 4:
                    char = parts[0]
                    self.data[char] = {
                        "å£°æ¯": parts[1] if len(parts) > 1 else "",
                        "éŸµéƒ¨": parts[2] if len(parts) > 2 else "",
                        "æ‹ŸéŸ³": parts[3] if len(parts) > 3 else "",
                        "source": "æ½˜æ‚Ÿäº‘ã€Šæ±‰è¯­å¤éŸ³æ‰‹å†Œã€‹"
                    }
    
    def _load_starostin(self, path: Path):
        """åŠ è½½æ–¯å¡”ç½—æ–¯é‡‘æ‹ŸéŸ³æ•°æ®"""
        # ç±»ä¼¼å¤„ç†ï¼Œæ ¹æ®å®é™…æ ¼å¼è°ƒæ•´
        pass
    
    def query(self, char: str) -> Optional[dict]:
        """æŸ¥è¯¢å•å­—çš„ä¸Šå¤éŸ³"""
        return self.data.get(char)
```

```python
# src/tools/phonology_tool.py

from typing import Dict
from ..knowledge.phonology_loader import PhonologyLoader


# éŸµéƒ¨ç›¸è¿‘å…³ç³»è¡¨
YUNBU_GROUPS = {
    # é˜´å£°éŸµ
    "ä¹‹": ["ä¹‹", "èŒ", "è’¸"],
    "æ”¯": ["æ”¯", "é”¡", "è€•"],
    "é±¼": ["é±¼", "é“", "é˜³"],
    "ä¾¯": ["ä¾¯", "å±‹", "ä¸œ"],
    "å®µ": ["å®µ", "è¯", "è§‰"],
    "å¹½": ["å¹½", "è§‰", "å†¬"],
    "å¾®": ["å¾®", "ç‰©", "æ–‡"],
    "æ­Œ": ["æ­Œ", "æœˆ", "å…ƒ"],
    # å…¥å£°éŸµ
    "èŒ": ["èŒ", "ä¹‹", "è’¸"],
    "é”¡": ["é”¡", "æ”¯", "è€•"],
    "é“": ["é“", "é±¼", "é˜³"],
    "å±‹": ["å±‹", "ä¾¯", "ä¸œ"],
    "è¯": ["è¯", "å®µ", "è§‰"],
    "ç‰©": ["ç‰©", "å¾®", "æ–‡"],
    "æœˆ": ["æœˆ", "æ­Œ", "å…ƒ"],
    # é˜³å£°éŸµ
    "è’¸": ["è’¸", "ä¹‹", "èŒ"],
    "è€•": ["è€•", "æ”¯", "é”¡"],
    "é˜³": ["é˜³", "é±¼", "é“"],
    "ä¸œ": ["ä¸œ", "ä¾¯", "å±‹"],
    "å†¬": ["å†¬", "å¹½", "è§‰"],
    "æ–‡": ["æ–‡", "å¾®", "ç‰©"],
    "å…ƒ": ["å…ƒ", "æ­Œ", "æœˆ"],
    "çœŸ": ["çœŸ", "è°†", "è‡»"],
    "è°ˆ": ["è°ˆ", "ç›", "æ·»"],
    "ä¾µ": ["ä¾µ", "è¦ƒ", "è°ˆ"],
}

# å£°æ¯ç›¸è¿‘å…³ç³»è¡¨
SHENGMU_GROUPS = {
    "å¸®": ["å¸®", "æ»‚", "å¹¶", "æ˜"],  # å”‡éŸ³
    "æ»‚": ["å¸®", "æ»‚", "å¹¶", "æ˜"],
    "å¹¶": ["å¸®", "æ»‚", "å¹¶", "æ˜"],
    "æ˜": ["å¸®", "æ»‚", "å¹¶", "æ˜"],
    
    "ç«¯": ["ç«¯", "é€", "å®š", "æ³¥"],  # èˆŒå¤´éŸ³
    "é€": ["ç«¯", "é€", "å®š", "æ³¥"],
    "å®š": ["ç«¯", "é€", "å®š", "æ³¥"],
    "æ³¥": ["ç«¯", "é€", "å®š", "æ³¥"],
    
    "ç²¾": ["ç²¾", "æ¸…", "ä»", "å¿ƒ", "é‚ª"],  # é½¿å¤´éŸ³
    "æ¸…": ["ç²¾", "æ¸…", "ä»", "å¿ƒ", "é‚ª"],
    "ä»": ["ç²¾", "æ¸…", "ä»", "å¿ƒ", "é‚ª"],
    "å¿ƒ": ["ç²¾", "æ¸…", "ä»", "å¿ƒ", "é‚ª"],
    "é‚ª": ["ç²¾", "æ¸…", "ä»", "å¿ƒ", "é‚ª"],
    
    "ç« ": ["ç« ", "æ˜Œ", "èˆ¹", "ä¹¦", "ç¦…"],  # æ­£é½¿éŸ³
    "æ˜Œ": ["ç« ", "æ˜Œ", "èˆ¹", "ä¹¦", "ç¦…"],
    "èˆ¹": ["ç« ", "æ˜Œ", "èˆ¹", "ä¹¦", "ç¦…"],
    "ä¹¦": ["ç« ", "æ˜Œ", "èˆ¹", "ä¹¦", "ç¦…"],
    "ç¦…": ["ç« ", "æ˜Œ", "èˆ¹", "ä¹¦", "ç¦…"],
    
    "è§": ["è§", "æºª", "ç¾¤", "ç–‘"],  # ç‰™éŸ³
    "æºª": ["è§", "æºª", "ç¾¤", "ç–‘"],
    "ç¾¤": ["è§", "æºª", "ç¾¤", "ç–‘"],
    "ç–‘": ["è§", "æºª", "ç¾¤", "ç–‘"],
    
    "å½±": ["å½±", "æ™“", "åŒ£", "å–»"],  # å–‰éŸ³
    "æ™“": ["å½±", "æ™“", "åŒ£", "å–»"],
    "åŒ£": ["å½±", "æ™“", "åŒ£", "å–»"],
    "å–»": ["å½±", "æ™“", "åŒ£", "å–»"],
    
    "æ¥": ["æ¥"],  # åŠèˆŒéŸ³
    "æ—¥": ["æ—¥"],  # åŠé½¿éŸ³
}


class PhonologyTool:
    """è¯­éŸ³åˆ†æå·¥å…· - Step 2"""
    
    def __init__(self, data_dir: str = None):
        if data_dir:
            self.loader = PhonologyLoader(data_dir)
        else:
            self.loader = None
    
    def query_phonology(self, char: str) -> dict:
        """æŸ¥è¯¢å­—çš„ä¸Šå¤éŸ³"""
        if self.loader:
            data = self.loader.query(char)
            if data:
                return {
                    "char": char,
                    "found": True,
                    **data
                }
        
        return {
            "char": char,
            "found": False,
            "å£°æ¯": "æœªæ”¶å½•",
            "éŸµéƒ¨": "æœªæ”¶å½•",
            "æ‹ŸéŸ³": "æœªæ”¶å½•"
        }
    
    def compare_phonology(self, char_a: str, char_b: str) -> dict:
        """æ¯”è¾ƒä¸¤å­—è¯­éŸ³å…³ç³»"""
        phon_a = self.query_phonology(char_a)
        phon_b = self.query_phonology(char_b)
        
        # åˆ¤æ–­éŸµéƒ¨æ˜¯å¦ç›¸è¿‘
        yunbu_a = phon_a.get("éŸµéƒ¨", "")
        yunbu_b = phon_b.get("éŸµéƒ¨", "")
        yunbu_close = self._is_yunbu_close(yunbu_a, yunbu_b)
        
        # åˆ¤æ–­å£°æ¯æ˜¯å¦ç›¸è¿‘
        shengmu_a = phon_a.get("å£°æ¯", "")
        shengmu_b = phon_b.get("å£°æ¯", "")
        shengmu_close = self._is_shengmu_close(shengmu_a, shengmu_b)
        
        # ç»¼åˆåˆ¤æ–­ï¼šéŸµéƒ¨ç›¸åŒæˆ–ç›¸è¿‘ + å£°æ¯ç›¸è¿‘ = éŸ³è¿‘
        is_close = yunbu_close and shengmu_close
        
        details = []
        if yunbu_a == yunbu_b:
            details.append(f"éŸµéƒ¨ç›¸åŒï¼ˆ{yunbu_a}éƒ¨ï¼‰")
        elif yunbu_close:
            details.append(f"éŸµéƒ¨ç›¸è¿‘ï¼ˆ{yunbu_a} / {yunbu_b}ï¼‰")
        else:
            details.append(f"éŸµéƒ¨ä¸åŒï¼ˆ{yunbu_a} / {yunbu_b}ï¼‰")
        
        if shengmu_a == shengmu_b:
            details.append(f"å£°æ¯ç›¸åŒï¼ˆ{shengmu_a}æ¯ï¼‰")
        elif shengmu_close:
            details.append(f"å£°æ¯ç›¸è¿‘ï¼ˆ{shengmu_a} / {shengmu_b}ï¼‰")
        else:
            details.append(f"å£°æ¯ä¸åŒï¼ˆ{shengmu_a} / {shengmu_b}ï¼‰")
        
        return {
            "char_a": char_a,
            "char_b": char_b,
            "phon_a": phon_a,
            "phon_b": phon_b,
            "relation": "éŸ³è¿‘" if is_close else "éŸ³è¿œ",
            "details": "ï¼›".join(details),
            "confidence": 0.9 if is_close else 0.85
        }
    
    def _is_yunbu_close(self, yunbu_a: str, yunbu_b: str) -> bool:
        """åˆ¤æ–­éŸµéƒ¨æ˜¯å¦ç›¸è¿‘"""
        if yunbu_a == yunbu_b:
            return True
        
        group_a = YUNBU_GROUPS.get(yunbu_a, [yunbu_a])
        return yunbu_b in group_a
    
    def _is_shengmu_close(self, shengmu_a: str, shengmu_b: str) -> bool:
        """åˆ¤æ–­å£°æ¯æ˜¯å¦ç›¸è¿‘"""
        if shengmu_a == shengmu_b:
            return True
        
        group_a = SHENGMU_GROUPS.get(shengmu_a, [shengmu_a])
        return shengmu_b in group_a
```

---

## ğŸ”§ Step 3: å¼‚æ–‡æ£€ç´¢å·¥å…·

### ç›®æ ‡

ä»è¯å…¸ä¸­æ£€ç´¢å¼‚æ–‡è®°å½•ã€å‡å€Ÿæ ‡æ³¨ç­‰ä½è¯ææ–™ã€‚

### å®ç°ä»£ç 

```python
# src/tools/textual_tool.py

import re
from typing import List, Dict
from ..knowledge.dictionary_loader import DictionaryLoader


class TextualTool:
    """å¼‚æ–‡ä¸æ–‡ä¾‹æ£€ç´¢å·¥å…· - Step 3"""
    
    def __init__(self, dictionary_path: str = None):
        if dictionary_path:
            self.dictionary = DictionaryLoader(dictionary_path)
        else:
            self.dictionary = None
    
    def search_evidence(self, char_a: str, char_b: str) -> dict:
        """
        æ£€ç´¢ä¸¤å­—çš„æ­£å€Ÿæ›¿ä»£å…³ç³»è¯æ®
        
        æ£€ç´¢å†…å®¹ï¼š
        1. è¯å…¸ä¸­çš„å¼‚æ–‡è®°å½•
        2. è¯å…¸ä¸­çš„å‡å€Ÿæ ‡æ³¨
        3. ä¾‹å¥ä¸­ä¸¤å­—äº’ç°
        """
        evidence = {
            "å¼‚æ–‡": [],
            "å‡å€Ÿæ ‡æ³¨": [],
            "ç›¸å…³ä¾‹å¥": []
        }
        
        if self.dictionary:
            # æŸ¥è¯¢ä¸¤å­—çš„è¯æ¡
            data_a = self.dictionary.query(char_a)
            data_b = self.dictionary.query(char_b)
            
            # 1. æ£€æŸ¥å‡å€Ÿæ ‡æ³¨
            if data_a:
                jiajie = self._find_jiajie_annotation(data_a, char_b)
                if jiajie:
                    evidence["å‡å€Ÿæ ‡æ³¨"].extend(jiajie)
            
            if data_b:
                jiajie = self._find_jiajie_annotation(data_b, char_a)
                if jiajie:
                    evidence["å‡å€Ÿæ ‡æ³¨"].extend(jiajie)
            
            # 2. æ£€æŸ¥ä¾‹å¥ä¸­çš„å¼‚æ–‡
            if data_a:
                yiwen = self._find_yiwen(data_a, char_b)
                if yiwen:
                    evidence["å¼‚æ–‡"].extend(yiwen)
        
        # åˆ¤æ–­æ˜¯å¦æœ‰ä½è¯
        has_evidence = (
            len(evidence["å¼‚æ–‡"]) > 0 or
            len(evidence["å‡å€Ÿæ ‡æ³¨"]) > 0
        )
        
        return {
            "char_a": char_a,
            "char_b": char_b,
            "has_evidence": has_evidence,
            "evidence": evidence,
            "summary": self._generate_summary(evidence)
        }
    
    def _find_jiajie_annotation(self, data: dict, target_char: str) -> List[str]:
        """åœ¨è¯æ¡ä¸­æŸ¥æ‰¾å‡å€Ÿæ ‡æ³¨"""
        results = []
        
        # æ£€æŸ¥ä¹‰é¡¹ä¸­æ˜¯å¦æœ‰å‡å€Ÿè¯´æ˜
        for sense in data.get('senses', []):
            definition = sense.get('definition', '')
            
            # å¸¸è§å‡å€Ÿæ ‡æ³¨æ¨¡å¼
            patterns = [
                rf"é€š[ã€Œã€Œ"]?{target_char}[ã€ã€"]?",
                rf"å‡å€Ÿ[ä¸ºç‚º]?[ã€Œã€Œ"]?{target_char}[ã€ã€"]?",
                rf"è¯»[ä¸ºç‚º]?[ã€Œã€Œ"]?{target_char}[ã€ã€"]?",
                rf"ä¸[ã€Œã€Œ"]?{target_char}[ã€ã€"]?é€š",
            ]
            
            for pattern in patterns:
                if re.search(pattern, definition):
                    results.append(definition[:100])
                    break
        
        return results
    
    def _find_yiwen(self, data: dict, target_char: str) -> List[str]:
        """åœ¨ä¾‹å¥ä¸­æŸ¥æ‰¾å¼‚æ–‡"""
        results = []
        
        for sense in data.get('senses', []):
            for example in sense.get('examples', []):
                quote = example.get('quote', '')
                source = example.get('source', '')
                
                # æ£€æŸ¥æ˜¯å¦æåˆ°å¼‚æ–‡
                if target_char in quote or "å¼‚æ–‡" in quote or "ä¸€ä½œ" in quote:
                    results.append(f"{source}: {quote[:50]}")
        
        return results
    
    def _generate_summary(self, evidence: dict) -> str:
        """ç”Ÿæˆè¯æ®æ‘˜è¦"""
        parts = []
        
        if evidence["å¼‚æ–‡"]:
            parts.append(f"æ‰¾åˆ°{len(evidence['å¼‚æ–‡'])}å¤„å¼‚æ–‡")
        
        if evidence["å‡å€Ÿæ ‡æ³¨"]:
            parts.append(f"æ‰¾åˆ°{len(evidence['å‡å€Ÿæ ‡æ³¨'])}å¤„å‡å€Ÿè®°å½•")
        
        if parts:
            return "ï¼›".join(parts)
        else:
            return "æœªæ‰¾åˆ°ç›¸å…³ä½è¯"
```

---

## ğŸ”§ Step 4: è®­å¼è¯†åˆ«å·¥å…·

### çŠ¶æ€ï¼šâœ… å·²å®ç°

è§ `src/tools/pattern_tool.py`ï¼ŒåŒ…å« 50+ è®­é‡Šæ ¼å¼çš„æ­£åˆ™åŒ¹é…ã€‚

### æ ¸å¿ƒä»£ç ç‰‡æ®µ

```python
# å·²å®ç°çš„è®­å¼æ¨¡å¼ï¼ˆéƒ¨åˆ†ï¼‰
XUNSHI_PATTERNS = {
    # ===== Aç±»ï¼šç›´æ¥åˆ¤å‡å€Ÿ =====
    "è¯»ä¸º": {
        "regex": r"(.+)[ï¼Œ,]\s*è¯»ä¸º\s*(.+)",
        "type": "å‡å€Ÿ",
        "confidence": "é«˜",
        "direct_judge": True
    },
    "è¯»æ›°": {
        "regex": r"(.+)[ï¼Œ,]\s*è¯»æ›°\s*(.+)",
        "type": "å‡å€Ÿ",
        "confidence": "é«˜",
        "direct_judge": True
    },
    "å½“ä¸º": {
        "regex": r"(.+)[ï¼Œ,]\s*å½“ä¸º\s*(.+)",
        "type": "å‡å€Ÿ",
        "confidence": "é«˜",
        "direct_judge": True
    },
    
    # ===== Bç±»ï¼šç›´æ¥åˆ¤è¯­ä¹‰ =====
    "çŠ¹...ä¹Ÿ": {
        "regex": r"(.+)[ï¼Œ,]\s*çŠ¹\s*(.+)\s*ä¹Ÿ",
        "type": "è¯­ä¹‰",
        "confidence": "é«˜",
        "direct_judge": True
    },
    
    # ===== Cç±»ï¼šéœ€ç»¼åˆåˆ¤æ–­ =====
    "Aä¹Ÿ": {
        "regex": r"(.+)[ï¼Œ,]\s*(.+)ä¹Ÿ$",
        "type": "ä¸ç¡®å®š",
        "confidence": "ä½",
        "direct_judge": False
    }
}
```

---

## ğŸ”§ Step 5: è¯­å¢ƒåˆ†æå·¥å…·

### ç›®æ ‡

å°†æœ¬ä¹‰ä»£å…¥åŸå¥ï¼Œåˆ¤æ–­è¯­ä¹‰æ˜¯å¦é€šé¡ºã€‚

### å®ç°ä»£ç 

```python
# src/tools/context_tool.py

from typing import Optional


class ContextTool:
    """è¯­å¢ƒé€‚é…åº¦åˆ†æå·¥å…· - Step 5"""
    
    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼Œç”¨äºè¯­ä¹‰åˆ†æ
        """
        self.llm = llm_client
    
    def analyze_context(
        self,
        beishi: str,
        shi: str,
        context: str,
        meaning_beishi: str = None,
        meaning_shi: str = None
    ) -> dict:
        """
        åˆ†æè¯­å¢ƒé€‚é…åº¦
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        - å°†è¢«é‡Šå­—çš„æœ¬ä¹‰ä»£å…¥åŸå¥ï¼Œæ£€æŸ¥æ˜¯å¦é€šé¡º
        - å°†é‡Šå­—çš„æœ¬ä¹‰ä»£å…¥åŸå¥ï¼Œæ£€æŸ¥æ˜¯å¦é€šé¡º
        - å€Ÿå­—æœ¬ä¹‰ä¸é€š + æ­£å­—æœ¬ä¹‰é€šé¡º = æ”¯æŒå‡å€Ÿ
        """
        if not context:
            return {
                "beishi": beishi,
                "shi": shi,
                "context": None,
                "conclusion": "æ— è¯­å¢ƒ",
                "reasoning": "æœªæä¾›åŸæ–‡è¯­å¢ƒï¼Œæ— æ³•è¿›è¡Œé€‚é…åº¦åˆ†æ"
            }
        
        # å¦‚æœæœ‰LLMï¼Œä½¿ç”¨LLMåˆ†æ
        if self.llm:
            return self._analyze_with_llm(
                beishi, shi, context, meaning_beishi, meaning_shi
            )
        
        # å¦åˆ™ä½¿ç”¨è§„åˆ™åˆ¤æ–­ï¼ˆç®€åŒ–ç‰ˆï¼‰
        return self._analyze_with_rules(
            beishi, shi, context, meaning_beishi, meaning_shi
        )
    
    def _analyze_with_llm(
        self,
        beishi: str,
        shi: str,
        context: str,
        meaning_beishi: str,
        meaning_shi: str
    ) -> dict:
        """ä½¿ç”¨LLMåˆ†æè¯­å¢ƒé€‚é…åº¦"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½å¤æ±‰è¯­ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹è¯­å¢ƒé€‚é…åº¦ï¼š

åŸæ–‡ï¼š{context}
è¢«é‡Šå­—ï¼š{beishi}ï¼Œæœ¬ä¹‰ï¼š{meaning_beishi}
é‡Šå­—ï¼š{shi}ï¼Œæœ¬ä¹‰ï¼š{meaning_shi}

è¯·åˆ¤æ–­ï¼š
1. å°†"{beishi}"çš„æœ¬ä¹‰"{meaning_beishi}"ä»£å…¥åŸå¥"{context}"ï¼Œè¯­ä¹‰æ˜¯å¦é€šé¡ºï¼Ÿ
2. å°†"{shi}"çš„æœ¬ä¹‰"{meaning_shi}"ä»£å…¥åŸå¥ï¼Œè¯­ä¹‰æ˜¯å¦é€šé¡ºï¼Ÿ

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
è¢«é‡Šå­—ä»£å…¥: [é€šé¡º/ä¸é€šé¡º]
é‡Šå­—ä»£å…¥: [é€šé¡º/ä¸é€šé¡º]
ç»“è®º: [æ”¯æŒå‡å€Ÿ/æ”¯æŒè¯­ä¹‰/ä¸ç¡®å®š]
åˆ†æ: [ç®€è¦è¯´æ˜]
"""
        
        # è°ƒç”¨LLM
        response = self.llm.generate(prompt)
        
        # è§£æå“åº”
        return self._parse_llm_response(response, beishi, shi, context)
    
    def _analyze_with_rules(
        self,
        beishi: str,
        shi: str,
        context: str,
        meaning_beishi: str,
        meaning_shi: str
    ) -> dict:
        """ä½¿ç”¨è§„åˆ™åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼Œä½œä¸ºå¤‡é€‰ï¼‰"""
        
        # ç®€å•æ›¿æ¢æ£€æŸ¥
        substituted_beishi = context.replace(beishi, f"({meaning_beishi})")
        substituted_shi = context.replace(beishi, f"({meaning_shi})")
        
        return {
            "beishi": beishi,
            "shi": shi,
            "context": context,
            "analysis": {
                "beishi_substituted": substituted_beishi,
                "shi_substituted": substituted_shi
            },
            "conclusion": "ä¸ç¡®å®š",
            "reasoning": "éœ€è¦äººå·¥åˆ¤æ–­æˆ–LLMè¾…åŠ©åˆ†æ"
        }
    
    def _parse_llm_response(self, response: str, beishi: str, shi: str, context: str) -> dict:
        """è§£æLLMå“åº”"""
        # ç®€åŒ–è§£æé€»è¾‘
        conclusion = "ä¸ç¡®å®š"
        
        if "æ”¯æŒå‡å€Ÿ" in response:
            conclusion = "æ”¯æŒå‡å€Ÿ"
        elif "æ”¯æŒè¯­ä¹‰" in response:
            conclusion = "æ”¯æŒè¯­ä¹‰"
        
        return {
            "beishi": beishi,
            "shi": shi,
            "context": context,
            "conclusion": conclusion,
            "reasoning": response
        }
```

---

## ğŸ”§ æ¥å…¥ LLM API

### Anthropic Claude

```python
# src/llm/claude_client.py

import os
from anthropic import Anthropic


class ClaudeClient:
    def __init__(self, api_key: str = None):
        self.client = Anthropic(
            api_key=api_key or os.getenv("ANTHROPIC_API_KEY")
        )
        self.model = "claude-3-5-sonnet-20241022"
    
    def generate(self, prompt: str, max_tokens: int = 1024) -> str:
        message = self.client.messages.create(
            model=self.model,
            max_tokens=max_tokens,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return message.content[0].text
```

### OpenAI GPT

```python
# src/llm/openai_client.py

import os
from openai import OpenAI


class OpenAIClient:
    def __init__(self, api_key: str = None):
        self.client = OpenAI(
            api_key=api_key or os.getenv("OPENAI_API_KEY")
        )
        self.model = "gpt-4-turbo-preview"
    
    def generate(self, prompt: str, max_tokens: int = 1024) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            max_tokens=max_tokens,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content
```

---

## ğŸ“Š æµ‹è¯•æ•°æ®é›†æ„å»º

### ä»å‚è€ƒèµ„æ–™æå–

```python
# scripts/extract_test_cases.py

import re

def extract_jiajie_examples(text: str) -> list:
    """ä»å‚è€ƒèµ„æ–™ä¸­æå–å‡å€Ÿç¤ºä¾‹"""
    examples = []
    
    # åŒ¹é…æ¨¡å¼ï¼šXï¼Œè¯»ä¸º/è¯»æ›°/å½“ä¸º Y
    patterns = [
        r"[ã€Œã€Œ](.+)[ï¼Œ,]\s*è¯»ä¸º\s*(.+)[ã€ã€]",
        r"[ã€Œã€Œ](.+)[ï¼Œ,]\s*è¯»æ›°\s*(.+)[ã€ã€]",
        r"ä¾‹å¦‚.+?[ã€Œã€Œ](.+)[ï¼Œ,]\s*(.+)ä¹Ÿ[ã€ã€].+?å‡å€Ÿ",
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            examples.append({
                "xungu_sentence": f"{match[0]}ï¼Œ{match[1]}",
                "beishi_char": match[0],
                "shi_char": match[1],
                "expected_label": "å‡å€Ÿè¯´æ˜"
            })
    
    return examples
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å¿«é€Ÿå¼€å§‹](QUICK_START.md)
- [ä»»åŠ¡è¦æ±‚](TASK_REQUIREMENTS.md)
- [æ¶æ„è®¾è®¡](ARCHITECTURE.md)
- [å¼€å‘è®¡åˆ’](DEVELOPMENT_PLAN.md)
