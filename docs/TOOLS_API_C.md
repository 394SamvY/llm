# å·¥å…·å±‚APIæ–‡æ¡£

> **è´Ÿè´£äºº**: æˆå‘˜C  
> **å®Œæˆæ—¥æœŸ**: 2026-01-17  
> **çŠ¶æ€**: âœ… å·²å®Œæˆå¹¶æ¥å…¥çœŸå®æ•°æ®

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [å‰ç½®æ¡ä»¶ï¼šæ„å»ºç´¢å¼•æ–‡ä»¶](#å‰ç½®æ¡ä»¶æ„å»ºç´¢å¼•æ–‡ä»¶)
3. [è¯­ä¹‰æŸ¥è¯¢å·¥å…· (semantic_tool.py)](#è¯­ä¹‰æŸ¥è¯¢å·¥å…·-semantic_toolpy)
4. [æ–‡çŒ®æ£€ç´¢å·¥å…· (textual_tool.py)](#æ–‡çŒ®æ£€ç´¢å·¥å…·-textual_toolpy)
5. [è®­å¼è¯†åˆ«å·¥å…· (pattern_tool.py)](#è®­å¼è¯†åˆ«å·¥å…·-pattern_toolpy)
6. [é›†æˆä½¿ç”¨ç¤ºä¾‹](#é›†æˆä½¿ç”¨ç¤ºä¾‹)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. æ„å»ºç´¢å¼•æ–‡ä»¶ï¼ˆå¿…é¡»ï¼ï¼‰

åœ¨ä½¿ç”¨å·¥å…·ä¹‹å‰ï¼Œå¿…é¡»å…ˆæ„å»ºè¯å…¸ç´¢å¼•æ–‡ä»¶ï¼š

```bash
# æ–¹æ³•1: ä½¿ç”¨ä¾¿æ·è„šæœ¬
python check_and_build_index.py

# æ–¹æ³•2: ç›´æ¥è¿è¡Œæ„å»ºå™¨
python -c "from src.data.dyhdc_index_builder import build_dyhdc_index; build_dyhdc_index()"
```

ç´¢å¼•æ–‡ä»¶å°†ç”Ÿæˆåœ¨ï¼š`data/processed/dyhdc_index.json`ï¼ˆçº¦24.5MBï¼‰

### 3. ä½¿ç”¨å·¥å…·

```python
from src.tools import query_word_meaning, search_textual_evidence, identify_pattern

# è¯­ä¹‰æŸ¥è¯¢
result = query_word_meaning("å´‡")
print(result["æœ¬ä¹‰"])

# æ–‡çŒ®æ£€ç´¢
evidence = search_textual_evidence("å´‡", "çµ‚", context="å´‡æœå…¶é›¨")
print(evidence["æœ‰ä½è¯"])

# è®­å¼è¯†åˆ«
pattern = identify_pattern("å´‡ï¼Œè®€ç‚ºçµ‚")
print(pattern["æš—ç¤ºç±»å‹"])  # "å‡å€Ÿ"
```

---

## å‰ç½®æ¡ä»¶ï¼šæ„å»ºç´¢å¼•æ–‡ä»¶

### ä¸ºä»€ä¹ˆéœ€è¦ç´¢å¼•æ–‡ä»¶ï¼Ÿ

ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹JSONLæ–‡ä»¶çº¦1.9GBï¼Œç›´æ¥éå†æŸ¥è¯¢éå¸¸æ…¢ã€‚ç´¢å¼•æ–‡ä»¶è®°å½•äº†æ¯ä¸ªå­—åœ¨JSONLæ–‡ä»¶ä¸­çš„ä½ç½®ï¼ˆåç§»é‡ï¼‰ï¼Œå¯ä»¥å¿«é€Ÿå®šä½å’Œè¯»å–ã€‚

### å¦‚ä½•æ„å»ºç´¢å¼•

**æ­¥éª¤1ï¼šæ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨**

```python
from pathlib import Path
from src.config import get_settings

settings = get_settings()
index_path = settings.data_processed_dir / "dyhdc_index.json"

if not index_path.exists():
    print("ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ„å»º")
else:
    print(f"ç´¢å¼•æ–‡ä»¶å·²å­˜åœ¨: {index_path}")
```

**æ­¥éª¤2ï¼šæ„å»ºç´¢å¼•**

```python
from src.data.dyhdc_index_builder import build_dyhdc_index

# è‡ªåŠ¨ä½¿ç”¨é»˜è®¤è·¯å¾„
build_dyhdc_index()

# æˆ–æŒ‡å®šè·¯å¾„
build_dyhdc_index(
    jsonl_path="ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹ç»“æ„åŒ–/dyhdc.parsed.fixed.v2.jsonl",
    output_path="data/processed/dyhdc_index.json"
)
```

**æ„å»ºæ—¶é—´**ï¼šçº¦1-3åˆ†é’Ÿï¼ˆå–å†³äºç£ç›˜é€Ÿåº¦ï¼‰

**è¾“å‡º**ï¼š
- `data/processed/dyhdc_index.json` - ç´¢å¼•æ–‡ä»¶ï¼ˆçº¦24.5MBï¼‰
- åŒ…å«408,931æ¡è¯æ¡ï¼Œ27,678ä¸ªé¦–å­—

---

## è¯­ä¹‰æŸ¥è¯¢å·¥å…· (semantic_tool.py)

### åŠŸèƒ½è¯´æ˜

æŸ¥è¯¢æ±‰å­—çš„æœ¬ä¹‰ã€ä¹‰é¡¹ã€ä¾‹å¥ã€å‡å€Ÿæ ‡æ³¨ç­‰è¯­ä¹‰ä¿¡æ¯ã€‚

**æ•°æ®æº**ï¼šã€Šæ±‰è¯­å¤§è¯å…¸ã€‹ï¼Œé€šè¿‡`DYHDCIndexLoader`æŸ¥è¯¢

### ä½¿ç”¨æ–¹å¼

#### æ–¹å¼1ï¼šå‡½æ•°å¼æ¥å£ï¼ˆæ¨èï¼‰

```python
from src.tools import query_word_meaning

# æŸ¥è¯¢å•ä¸ªå­—
result = query_word_meaning("å´‡")

print(result["å­—"])          # "å´‡"
print(result["æœ¬ä¹‰"])        # "é«˜ï¼›é«˜å¤§ã€‚"
print(result["ä¹‰é¡¹"])        # ["é«˜ï¼›é«˜å¤§ã€‚", "å°Šå´‡", "å……å®", ...]
print(result["ä¾‹å¥"])        # [{"quote": "å´‡å±±å³»å²­"}, ...]
print(result["å‡å€Ÿæ ‡æ³¨"])    # ["12é€š"çµ‚"ã€‚ç»ˆå°½ã€‚å‚è§"å´‡æœ"ã€‚"]
```

#### æ–¹å¼2ï¼šç±»å¼æ¥å£

```python
from src.tools import SemanticTool

tool = SemanticTool()
word_meaning = tool.query("å´‡")

print(word_meaning.char)              # "å´‡"
print(word_meaning.primary_meaning)   # "é«˜ï¼›é«˜å¤§ã€‚"
print(word_meaning.meanings)          # ["é«˜ï¼›é«˜å¤§ã€‚", "å°Šå´‡", ...]
print(word_meaning.jiajie_notes)      # ["12é€š"çµ‚"ã€‚ç»ˆå°½ã€‚å‚è§"å´‡æœ"ã€‚"]
```

### è¿”å›æ•°æ®æ ¼å¼

**å‡½æ•°å¼æ¥å£è¿”å›**ï¼š
```python
{
    "å­—": "å´‡",
    "æœ¬ä¹‰": "é«˜ï¼›é«˜å¤§ã€‚",
    "ä¹‰é¡¹": [
        "é«˜ï¼›é«˜å¤§ã€‚",
        "å°Šå´‡",
        "å……å®",
        ...
    ],
    "ä¾‹å¥": [
        {"quote": "å´‡å±±å³»å²­"},
        ...
    ],
    "å‡å€Ÿæ ‡æ³¨": [
        "12é€š"çµ‚"ã€‚ç»ˆå°½ã€‚å‚è§"å´‡æœ"ã€‚"
    ]
}
```

**ç±»å¼æ¥å£è¿”å›**ï¼š`WordMeaning` å¯¹è±¡
```python
@dataclass
class WordMeaning:
    char: str                    # æ±‰å­—
    primary_meaning: str          # æœ¬ä¹‰
    meanings: List[str]          # æ‰€æœ‰ä¹‰é¡¹
    examples: List[Dict]         # ä¾‹å¥
    jiajie_notes: List[str]      # å‡å€Ÿæ ‡æ³¨
    raw_data: Optional[Dict]    # åŸå§‹æ•°æ®
```

### æ³¨æ„äº‹é¡¹

1. **ç¹ç®€ä½“é—®é¢˜**ï¼šè¯å…¸ä½¿ç”¨ç¹ä½“å­—ç´¢å¼•ï¼ŒæŸ¥è¯¢"ç»ˆ"éœ€ç”¨"çµ‚"
2. **æœªæ”¶å½•å¤„ç†**ï¼šå¦‚æœå­—ä¸åœ¨è¯å…¸ä¸­ï¼Œè¿”å›`"æœ¬ä¹‰": "æœªæ”¶å½•"`
3. **é¦–æ¬¡åŠ è½½**ï¼šé¦–æ¬¡æŸ¥è¯¢ä¼šåŠ è½½ç´¢å¼•ï¼ˆçº¦1ç§’ï¼‰ï¼Œåç»­æŸ¥è¯¢å¾ˆå¿«

### é”™è¯¯å¤„ç†

```python
result = query_word_meaning("å´‡")
if result["æœ¬ä¹‰"] == "æœªæ”¶å½•":
    print("è¯¥å­—æœªæ”¶å½•åœ¨è¯å…¸ä¸­")
elif result["æœ¬ä¹‰"] == "æœªåŠ è½½":
    print("ç´¢å¼•æ–‡ä»¶æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
```

---

## æ–‡çŒ®æ£€ç´¢å·¥å…· (textual_tool.py)

### åŠŸèƒ½è¯´æ˜

æ£€ç´¢ä¸¤ä¸ªå­—ä¹‹é—´çš„æ–‡çŒ®ä½è¯ï¼ŒåŒ…æ‹¬ï¼š
- å‡å€Ÿè®°å½•ï¼ˆè¯å…¸ä¸­çš„å‡å€Ÿæ ‡æ³¨ï¼‰
- å¼‚æ–‡ï¼ˆä¸åŒç‰ˆæœ¬çš„å­—å½¢å·®å¼‚ï¼‰
- å¹³è¡Œæ–‡æœ¬ï¼ˆç›¸ä¼¼ç”¨æ³•çš„æ–‡æœ¬ï¼‰

**æ•°æ®æº**ï¼šã€Šæ±‰è¯­å¤§è¯å…¸ã€‹çš„å‡å€Ÿæ ‡æ³¨å’Œä¾‹å¥

### ä½¿ç”¨æ–¹å¼

#### æ–¹å¼1ï¼šå‡½æ•°å¼æ¥å£ï¼ˆæ¨èï¼‰

```python
from src.tools import search_textual_evidence

# æ£€ç´¢ä¸¤ä¸ªå­—ä¹‹é—´çš„ä½è¯
result = search_textual_evidence(
    char_a="å´‡",      # è¢«é‡Šå­—
    char_b="çµ‚",      # é‡Šå­—
    context="å´‡æœå…¶é›¨"  # ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
)

print(result["æœ‰ä½è¯"])      # True/False
print(result["å‡å€Ÿè®°å½•"])    # [...]
print(result["å¼‚æ–‡"])        # [...]
print(result["æ€»ç»“"])        # "æ‰¾åˆ°2å¤„å‡å€Ÿè®°å½•"
```

#### æ–¹å¼2ï¼šç±»å¼æ¥å£

```python
from src.tools import TextualTool

tool = TextualTool()
evidence = tool.search("å´‡", "çµ‚", context="å´‡æœå…¶é›¨")

print(evidence.has_evidence)      # True
print(evidence.jiajie_records)    # [...]
print(evidence.variant_texts)     # [...]
print(evidence.summary)           # "æ‰¾åˆ°2å¤„å‡å€Ÿè®°å½•"
```

### è¿”å›æ•°æ®æ ¼å¼

**å‡½æ•°å¼æ¥å£è¿”å›**ï¼š
```python
{
    "æœ‰ä½è¯": True,
    "å‡å€Ÿè®°å½•": [
        {
            "type": "jiajie",
            "source": "ã€Šæ¯›ä¼ ã€‹",
            "text": "å´‡ï¼Œçµ‚ä¹Ÿ",
            "note": "è¯å…¸ä¸­æ ‡æ³¨ï¼šå´‡ä¸çµ‚çš„å‡å€Ÿå…³ç³»"
        }
    ],
    "å¼‚æ–‡": [
        {
            "type": "variant",
            "source": "ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹ä¾‹å¥",
            "text": "å´‡æœ vs çµ‚æœ",
            "note": "ä¾‹å¥ä¸­åŒ…å«ç›¸å…³ç”¨å­—"
        }
    ],
    "å¹³è¡Œæ–‡æœ¬": [],
    "æ€»ç»“": "æ‰¾åˆ°2å¤„å‡å€Ÿè®°å½•ï¼›æ‰¾åˆ°1å¤„å¼‚æ–‡"
}
```

**ç±»å¼æ¥å£è¿”å›**ï¼š`TextualEvidence` å¯¹è±¡
```python
@dataclass
class TextualEvidence:
    has_evidence: bool                    # æ˜¯å¦æ‰¾åˆ°ä½è¯
    variant_texts: List[Dict[str, str]]   # å¼‚æ–‡
    parallel_texts: List[Dict[str, str]]  # å¹³è¡Œæ–‡æœ¬
    jiajie_records: List[Dict[str, str]]  # å‡å€Ÿè®°å½•
    summary: str                          # æ€»ç»“è¯´æ˜
```

### æ£€ç´¢é€»è¾‘

1. **å‡å€Ÿæ ‡æ³¨æ£€ç´¢**ï¼š
   - ä»è¢«é‡Šå­—çš„å‡å€Ÿæ ‡æ³¨ä¸­æŸ¥æ‰¾æ˜¯å¦åŒ…å«é‡Šå­—
   - ä»é‡Šå­—çš„å‡å€Ÿæ ‡æ³¨ä¸­æŸ¥æ‰¾æ˜¯å¦åŒ…å«è¢«é‡Šå­—ï¼ˆåå‘ï¼‰

2. **å¼‚æ–‡æ£€ç´¢**ï¼š
   - å¦‚æœæä¾›äº†ä¸Šä¸‹æ–‡ï¼Œåœ¨ä¾‹å¥ä¸­æŸ¥æ‰¾åŒ…å«ä¸Šä¸‹æ–‡çš„ä¾‹å­
   - æŸ¥æ‰¾ä¾‹å¥ä¸­åŒæ—¶åŒ…å«ä¸¤ä¸ªå­—çš„æƒ…å†µ

3. **ä¹‰é¡¹æ£€ç´¢**ï¼š
   - åœ¨ä¹‰é¡¹ä¸­æŸ¥æ‰¾åŒ…å«"é€š"ã€"è¯»ä¸º"ã€"è¯»æ›°"ã€"å‡å€Ÿ"ç­‰æœ¯è¯­çš„æ¡ç›®

### æ³¨æ„äº‹é¡¹

1. **ä¸Šä¸‹æ–‡çš„ä½œç”¨**ï¼šæä¾›ä¸Šä¸‹æ–‡å¯ä»¥æé«˜æ£€ç´¢ç²¾åº¦
2. **ç¹ç®€ä½“**ï¼šå»ºè®®ä½¿ç”¨ç¹ä½“å­—æŸ¥è¯¢ï¼ŒåŒ¹é…æ›´å‡†ç¡®
3. **ç©ºç»“æœ**ï¼šå¦‚æœæœªæ‰¾åˆ°ä½è¯ï¼Œ`has_evidence`ä¸º`False`ï¼Œä½†è¿™æ˜¯æ­£å¸¸æƒ…å†µ

---

## è®­å¼è¯†åˆ«å·¥å…· (pattern_tool.py)

### åŠŸèƒ½è¯´æ˜

è¯†åˆ«è®­è¯‚å¥çš„æ ¼å¼ï¼Œåˆ¤æ–­ä½¿ç”¨äº†ä»€ä¹ˆè®­é‡Šæœ¯è¯­ï¼Œå¹¶æ¨æ–­æš—ç¤ºçš„ç±»å‹ï¼ˆå‡å€Ÿ/è¯­ä¹‰è§£é‡Š/ä»¥å£°é€šä¹‰ï¼‰ã€‚

**æ•°æ®æº**ï¼šå†…ç½®çš„è®­å¼è§„åˆ™è¡¨ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰

### ä½¿ç”¨æ–¹å¼

#### æ–¹å¼1ï¼šå‡½æ•°å¼æ¥å£ï¼ˆæ¨èï¼‰

```python
from src.tools import identify_pattern

# è¯†åˆ«è®­è¯‚å¥æ ¼å¼
result = identify_pattern("å´‡ï¼Œè®€ç‚ºçµ‚")

print(result["æ ¼å¼"])          # "è¯»ä¸º"
print(result["è¢«é‡Šå­—"])        # "å´‡"
print(result["é‡Šå­—"])          # "çµ‚"
print(result["æš—ç¤ºç±»å‹"])      # "å‡å€Ÿ"
print(result["ç½®ä¿¡åº¦"])        # "é«˜"
print(result["å¯ç›´æ¥åˆ¤å®š"])    # True
print(result["è¯´æ˜"])          # "éƒ‘ç„ã€Šç¤¼ã€‹æ³¨ï¼Œç ´å­—/æ”¹è¯»æœ¯è¯­"
```

#### æ–¹å¼2ï¼šç±»å¼æ¥å£

```python
from src.tools import PatternTool

tool = PatternTool()
pattern = tool.identify("å´‡ï¼Œçµ‚ä¹Ÿ")

print(pattern.pattern_name)      # "Aä¹Ÿ"
print(pattern.char_a)            # "å´‡"
print(pattern.char_b)            # "çµ‚"
print(pattern.implied_type)      # "ä¸ç¡®å®š"
print(pattern.confidence)        # "ä½"
print(pattern.can_direct_judge)  # False
```

### è¿”å›æ•°æ®æ ¼å¼

**å‡½æ•°å¼æ¥å£è¿”å›**ï¼š
```python
{
    "æ ¼å¼": "è¯»ä¸º",
    "è¢«é‡Šå­—": "å´‡",
    "é‡Šå­—": "çµ‚",
    "æš—ç¤ºç±»å‹": "å‡å€Ÿ",
    "ç½®ä¿¡åº¦": "é«˜",
    "å¯ç›´æ¥åˆ¤å®š": True,
    "è¯´æ˜": "éƒ‘ç„ã€Šç¤¼ã€‹æ³¨ï¼Œç ´å­—/æ”¹è¯»æœ¯è¯­"
}
```

**ç±»å¼æ¥å£è¿”å›**ï¼š`PatternResult` å¯¹è±¡
```python
@dataclass
class PatternResult:
    pattern_name: str        # æ ¼å¼åç§°
    char_a: str             # è¢«é‡Šå­—
    char_b: str             # é‡Šå­—
    implied_type: str       # æš—ç¤ºç±»å‹
    confidence: str         # ç½®ä¿¡åº¦
    can_direct_judge: bool  # æ˜¯å¦å¯ç›´æ¥åˆ¤å®š
    source: str            # æ¥æºè¯´æ˜
```

### æ”¯æŒçš„è®­å¼ç±»å‹

#### Aç±»ï¼šç›´æ¥åˆ¤å‡å€Ÿï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
- `è¯»ä¸º`ã€`è¯»æ›°`ã€`è¯»å½“ä¸º`ã€`å½“ä¸º`ã€`å½“ä½œ`
- `é€š`ã€`å¤å­—é€š`ã€`å‡å€Ÿå­—`ã€`å€Ÿå­—ä¹Ÿ`ã€`å€Ÿä¸º`

#### Bç±»ï¼šå¯èƒ½å‡å€Ÿï¼ˆä¸­ç½®ä¿¡åº¦ï¼‰
- `è¯»è‹¥`ã€`è¯»å¦‚`ã€`æˆ–ä½œ`ã€`äº¦ä½œ`ã€`æœ¬ä½œ`ã€`å£°è¿‘`

#### Cç±»ï¼šä»¥å£°é€šä¹‰
- `ä¹‹è¨€`ã€`ä¹‹ä¸ºè¨€`

#### Dç±»ï¼šç›´æ¥åˆ¤è¯­ä¹‰è§£é‡Š
- `çŠ¹ä¹Ÿ`ã€`çŠ¹è¨€`ã€`è°“ä¹‹`ã€`ä¹‹è²Œ`ã€`ä¹‹ç§°`ã€`è²Œ`ã€`æ‰€ä»¥`

#### Eç±»ï¼šä¸ç¡®å®šï¼ˆä½ç½®ä¿¡åº¦ï¼‰
- `è€…ä¹Ÿ`ã€`å³`ã€`Aä¹Ÿ`ï¼ˆåŸºæœ¬æ ¼å¼ï¼‰

### ç¤ºä¾‹

```python
# å‡å€Ÿç±»
identify_pattern("å´‡ï¼Œè®€ç‚ºçµ‚")      # ç±»å‹: "å‡å€Ÿ", ç½®ä¿¡åº¦: "é«˜"
identify_pattern("æ­£ï¼Œè®€ç‚ºå¾")      # ç±»å‹: "å‡å€Ÿ", ç½®ä¿¡åº¦: "é«˜"
identify_pattern("å´‡èˆ‡çµ‚é€š")        # ç±»å‹: "å‡å€Ÿ", ç½®ä¿¡åº¦: "é«˜"

# è¯­ä¹‰è§£é‡Šç±»
identify_pattern("å¤­å¤­ï¼Œç››ä¹Ÿ")      # ç±»å‹: "è¯­ä¹‰è§£é‡Š", ç½®ä¿¡åº¦: "é«˜"
identify_pattern("ç¡•ï¼Œå¤§è²Œ")        # ç±»å‹: "è¯­ä¹‰è§£é‡Š", ç½®ä¿¡åº¦: "é«˜"
identify_pattern("é‰´ï¼Œæ‰€ä»¥å¯Ÿå½¢ä¹Ÿ")  # ç±»å‹: "è¯­ä¹‰è§£é‡Š", ç½®ä¿¡åº¦: "é«˜"

# ä»¥å£°é€šä¹‰
identify_pattern("æµ·ä¹‹è¨€æ™¦ä¹Ÿ")      # ç±»å‹: "ä»¥å£°é€šä¹‰", ç½®ä¿¡åº¦: "ä¸­"

# ä¸ç¡®å®š
identify_pattern("å´‡ï¼Œçµ‚ä¹Ÿ")        # ç±»å‹: "ä¸ç¡®å®š", ç½®ä¿¡åº¦: "ä½"
identify_pattern("æ”¿è€…ï¼Œæ­£ä¹Ÿ")      # ç±»å‹: "ä¸ç¡®å®š", ç½®ä¿¡åº¦: "ä½"
```

### æ³¨æ„äº‹é¡¹

1. **æ ‡ç‚¹ç¬¦å·**ï¼šæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹ï¼ˆï¼Œ,ï¼‰
2. **ç¹ç®€ä½“**ï¼šè‡ªåŠ¨å¤„ç†ç¹ç®€ä½“
3. **ä¼˜å…ˆçº§**ï¼šæŒ‰Aç±»â†’Bç±»â†’Cç±»â†’Dç±»â†’Eç±»çš„é¡ºåºåŒ¹é…ï¼Œå…ˆåŒ¹é…åˆ°å°±è¿”å›
4. **ä½ç½®ä¿¡åº¦**ï¼šå¯¹äº"ä¸ç¡®å®š"ç±»å‹ï¼Œéœ€è¦ç»“åˆå…¶ä»–å·¥å…·ç»¼åˆåˆ¤æ–­

---

## é›†æˆä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´åˆ†ææµç¨‹

```python
from src.tools import (
    query_word_meaning,
    search_textual_evidence,
    identify_pattern
)

# åˆ†æè®­è¯‚å¥ï¼š"å´‡ï¼Œçµ‚ä¹Ÿ"
xungu_sentence = "å´‡ï¼Œçµ‚ä¹Ÿ"
char_a = "å´‡"
char_b = "çµ‚"
context = "å´‡æœå…¶é›¨"

print("=" * 60)
print(f"åˆ†æè®­è¯‚å¥: {xungu_sentence}")
print("=" * 60)

# æ­¥éª¤1: è¯­ä¹‰åˆ†æ
print("\n[1] è¯­ä¹‰åˆ†æ:")
meaning_a = query_word_meaning(char_a)
meaning_b = query_word_meaning(char_b)
print(f"  {char_a}çš„æœ¬ä¹‰: {meaning_a['æœ¬ä¹‰']}")
print(f"  {char_b}çš„æœ¬ä¹‰: {meaning_b['æœ¬ä¹‰']}")
print(f"  è¯­ä¹‰å…³ç³»: {'ä¹‰è¿‘' if meaning_a['æœ¬ä¹‰'] != 'æœªæ”¶å½•' and meaning_b['æœ¬ä¹‰'] != 'æœªæ”¶å½•' else 'æœªçŸ¥'}")

# æ­¥éª¤2: æ–‡çŒ®æ£€ç´¢
print("\n[2] æ–‡çŒ®æ£€ç´¢:")
evidence = search_textual_evidence(char_a, char_b, context)
print(f"  æœ‰ä½è¯: {evidence['æœ‰ä½è¯']}")
if evidence['å‡å€Ÿè®°å½•']:
    print(f"  å‡å€Ÿè®°å½•: {len(evidence['å‡å€Ÿè®°å½•'])}æ¡")
    print(f"  ç¬¬ä¸€æ¡: {evidence['å‡å€Ÿè®°å½•'][0]['text']}")

# æ­¥éª¤3: è®­å¼è¯†åˆ«
print("\n[3] è®­å¼è¯†åˆ«:")
pattern = identify_pattern(xungu_sentence)
print(f"  æ ¼å¼: {pattern['æ ¼å¼']}")
print(f"  æš—ç¤ºç±»å‹: {pattern['æš—ç¤ºç±»å‹']}")
print(f"  ç½®ä¿¡åº¦: {pattern['ç½®ä¿¡åº¦']}")
print(f"  å¯ç›´æ¥åˆ¤å®š: {pattern['å¯ç›´æ¥åˆ¤å®š']}")

# ç»¼åˆåˆ¤æ–­
print("\n[4] ç»¼åˆåˆ¤æ–­:")
if pattern['å¯ç›´æ¥åˆ¤å®š']:
    if pattern['æš—ç¤ºç±»å‹'] == 'å‡å€Ÿ':
        print("  â†’ åˆ¤æ–­ä¸º: å‡å€Ÿè¯´æ˜")
    elif pattern['æš—ç¤ºç±»å‹'] == 'è¯­ä¹‰è§£é‡Š':
        print("  â†’ åˆ¤æ–­ä¸º: è¯­ä¹‰è§£é‡Š")
else:
    print("  â†’ éœ€è¦ç»“åˆå…¶ä»–ä¿¡æ¯ç»¼åˆåˆ¤æ–­")
```

### æ‰¹é‡å¤„ç†æµ‹è¯•é›†

```python
from src.evaluation import load_test_dataset
from src.tools import query_word_meaning, search_textual_evidence, identify_pattern

# åŠ è½½æµ‹è¯•é›†
dataset = load_test_dataset("data/test/test_dataset.json")

# åˆ†ææ¯ä¸ªæµ‹è¯•ç”¨ä¾‹
for case in dataset:
    print(f"\n{'='*60}")
    print(f"ID: {case.id}")
    print(f"è®­è¯‚å¥: {case.xungu_sentence}")
    print(f"æœŸæœ›: {case.expected_label}")
    
    # ä½¿ç”¨ä¸‰ä¸ªå·¥å…·åˆ†æ
    meaning_a = query_word_meaning(case.char_a)
    meaning_b = query_word_meaning(case.char_b)
    evidence = search_textual_evidence(case.char_a, case.char_b, case.context)
    pattern = identify_pattern(case.xungu_sentence)
    
    print(f"è¯­ä¹‰: {meaning_a['æœ¬ä¹‰'][:30]}...")
    print(f"ä½è¯: {evidence['æœ‰ä½è¯']}")
    print(f"æ ¼å¼: {pattern['æ ¼å¼']}, ç±»å‹: {pattern['æš—ç¤ºç±»å‹']}")
```

---

## å¸¸è§é—®é¢˜

### Q1: æŸ¥è¯¢è¿”å›"æœªæ”¶å½•"æ€ä¹ˆåŠï¼Ÿ

**åŸå› **ï¼š
1. ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆæœ€å¸¸è§ï¼‰
2. ä½¿ç”¨äº†ç®€ä½“å­—ï¼Œä½†è¯å…¸ä½¿ç”¨ç¹ä½“å­—ç´¢å¼•

**è§£å†³æ–¹æ³•**ï¼š
```python
# 1. æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
from src.config import get_settings
settings = get_settings()
index_path = settings.data_processed_dir / "dyhdc_index.json"
print(f"ç´¢å¼•æ–‡ä»¶å­˜åœ¨: {index_path.exists()}")

# 2. å¦‚æœä¸å­˜åœ¨ï¼Œæ„å»ºç´¢å¼•
if not index_path.exists():
    from src.data.dyhdc_index_builder import build_dyhdc_index
    build_dyhdc_index()

# 3. ä½¿ç”¨ç¹ä½“å­—æŸ¥è¯¢
result = query_word_meaning("çµ‚")  # ä½¿ç”¨ç¹ä½“
```

### Q2: ç´¢å¼•æ–‡ä»¶æ„å»ºå¤±è´¥ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
1. JSONLæ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®
2. ç£ç›˜ç©ºé—´ä¸è¶³
3. æ–‡ä»¶æƒé™é—®é¢˜

**è§£å†³æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥JSONLæ–‡ä»¶
from src.config import get_settings
settings = get_settings()
jsonl_path = settings.dyhdc_path
print(f"JSONLæ–‡ä»¶: {jsonl_path}")
print(f"å­˜åœ¨: {jsonl_path.exists() if jsonl_path else False}")

# æ‰‹åŠ¨æŒ‡å®šè·¯å¾„æ„å»º
from src.data.dyhdc_index_builder import DYHDCIndexBuilder
builder = DYHDCIndexBuilder("ã€Šæ±‰è¯­å¤§è¯å…¸ã€‹ç»“æ„åŒ–/dyhdc.parsed.fixed.v2.jsonl")
builder.build_index("data/processed/dyhdc_index.json")
```

### Q3: å¦‚ä½•ç»™å…¶ä»–æˆå‘˜æä¾›æ¥å£ï¼Ÿ

**æ–¹å¼1ï¼šç›´æ¥å¯¼å…¥å‡½æ•°ï¼ˆæ¨èï¼‰**

```python
# åœ¨Agentæˆ–å…¶ä»–æ¨¡å—ä¸­
from src.tools import (
    query_word_meaning,      # è¯­ä¹‰æŸ¥è¯¢
    search_textual_evidence, # æ–‡çŒ®æ£€ç´¢
    identify_pattern         # è®­å¼è¯†åˆ«
)

# ç›´æ¥ä½¿ç”¨
result = query_word_meaning("å´‡")
```

**æ–¹å¼2ï¼šä½¿ç”¨ç±»æ¥å£**

```python
from src.tools import SemanticTool, TextualTool, PatternTool

semantic = SemanticTool()
textual = TextualTool()
pattern = PatternTool()

# ä½¿ç”¨
meaning = semantic.query("å´‡")
evidence = textual.search("å´‡", "çµ‚")
pattern_result = pattern.identify("å´‡ï¼Œçµ‚ä¹Ÿ")
```

**æ–¹å¼3ï¼šåœ¨Agentä¸­é›†æˆ**

```python
# åœ¨ xungu_agent.py ä¸­
from src.tools import SemanticTool, TextualTool, PatternTool

class XunguAgent:
    def __init__(self):
        self.semantic_tool = SemanticTool()
        self.textual_tool = TextualTool()
        self.pattern_tool = PatternTool()
    
    def analyze(self, sentence, char_a, char_b, context=None):
        # æ­¥éª¤1: è¯­ä¹‰åˆ†æ
        meaning_a = self.semantic_tool.query(char_a)
        meaning_b = self.semantic_tool.query(char_b)
        
        # æ­¥éª¤2: æ–‡çŒ®æ£€ç´¢
        evidence = self.textual_tool.search(char_a, char_b, context)
        
        # æ­¥éª¤3: è®­å¼è¯†åˆ«
        pattern = self.pattern_tool.identify(sentence)
        
        return {
            "semantic": {"a": meaning_a, "b": meaning_b},
            "evidence": evidence,
            "pattern": pattern
        }
```

### Q4: æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å•ä¾‹æ¨¡å¼**ï¼šå·¥å…·ç±»å·²å®ç°å•ä¾‹ï¼Œå¤šæ¬¡è°ƒç”¨ä¸ä¼šé‡å¤åŠ è½½ç´¢å¼•
2. **æ‰¹é‡æŸ¥è¯¢**ï¼šå¦‚æœéœ€è¦æŸ¥è¯¢å¤šä¸ªå­—ï¼Œå¯ä»¥å¤ç”¨åŒä¸€ä¸ªå·¥å…·å®ä¾‹
3. **ç´¢å¼•ç¼“å­˜**ï¼šç´¢å¼•æ–‡ä»¶åªéœ€æ„å»ºä¸€æ¬¡ï¼Œåç»­ç›´æ¥ä½¿ç”¨

```python
# å¥½çš„åšæ³•ï¼šå¤ç”¨å·¥å…·å®ä¾‹
tool = SemanticTool()
results = [tool.query(char) for char in ["å´‡", "çµ‚", "æµ·", "æ™¦"]]

# é¿å…ï¼šæ¯æ¬¡éƒ½åˆ›å»ºæ–°å®ä¾‹ï¼ˆè™½ç„¶å•ä¾‹æ¨¡å¼å·²å¤„ç†ï¼Œä½†æ˜¾å¼å¤ç”¨æ›´æ¸…æ™°ï¼‰
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
2. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯
3. å‚è€ƒæœ¬æ–‡æ¡£çš„å¸¸è§é—®é¢˜éƒ¨åˆ†
4. è”ç³»æˆå‘˜Cæˆ–æˆå‘˜E

---

*æœ€åæ›´æ–°ï¼š2026-01-17*
